[["index.html", "Statistics with jamovi Welcome", " Statistics with jamovi Dana Linnell Last Update: 2024-03-03 Welcome This is the website for PSYC 290 and PSYC 790 at the University of Wisconsin-Stout, taught by Dr. Dana Linnell. These resources are aimed at teaching you how to use jamovi and null hypothesis significance testing (NHST) to answer research questions. This website is free to use and is licensed under a Creative Commons BY-SA (CC BY-SA) license version 4.0. This means you are free to share (i.e., copy and redistribute the material in any medium or format) and free to adapt (i.e., remix, transform, and build upon the material for any purpose, even commercially), provided that you attribute these resources by citing me, indicating if changes were made and you share alike (i.e., if you adapt, you must distribute your contributions under the same license as the original). Many of the data examples come from “Learning statistics with jamovi: A tutorial for psychology students and other beginners” by Danielle J. Navarro and David R. Foxcroft, version 0.70. Dedication: This book is dedicated to my graduate statistics professor Dr. Dale Berger, who gave us a similar set of resources when he taught statistics at Claremont Graduate University. I still have my binder of handouts, homework assignments, and notes, which have been instrumental throughout my career. Thank you for showing me the joy of statistics! Image of Dale Berger and Dana Linnell at her master’s graduation ceremony "],["introduction.html", "1. Introduction", " 1. Introduction This chapter will introduce you to the course, the instructor (Dr. Dana Linnell), and the textbook. 1.1 Getting help in this class Come to student hours regularly! They are often available both in-person or online, depending on whether you are enrolled in an in-person or online section of the course. I am always available to help you. Depending on the semester, there may also be a graduate assistant (GA) who can support you as well. We will be scheduling regularly recurring student hours each week so you can come ask questions, get help on your homework, or just have a space to come together to work on your assignments in a dedicated online space. If you have questions, you can message me on Teams or email me at linnellda@uwstout.edu. See your course on Canvas if you have any questions about Teams. 1.2 Dana, your instructor My name is Dana Linnell (pronounced DAY-nuh lih-NELL) and I started teaching at UW-Stout in Fall 2019. I teach statistics, research methods, interpersonal effectiveness, and evaluation in the Department of Psychology. I love statistics! It is one way we can answer our research questions and test our hypotheses. However, I know not everyone likes statistics. Some of you may not care much about it, and some of you may be apprehensive or anxious about taking this course. Please know that I am here for you and I want to make this class an enjoyable learning experience. If there is anything I can do to help make this class more enjoyable and to help you learn, please reach out to me. 1.3 Navigating this website/book This book was developed using a coding language called R in a graphical user interface called RStudio using a package called bookdown. This book is hosted on a platform called GitHub. You can see the code for this book here if you are interested. There are some icons at the top of this book that you may find useful: The first icon of the toolbar, which looks like three horizontal lines, toggles the visibility of the sidebar, which contains the table of contents. You can also hit the S key on your keyboard to toggle the sidebar. The second icon of the toolbar, which looks like a magnifying glass, is the search feature, which you can use to search the entire book. You can also hit the F (Find) key on your keyboard. The third icon, which is a capital A, is for font/theme settings, which you can use to change font size (smaller or bigger), font family (serif or sans serif), and theme (white, sepia, or night). The fourth icon, which is a lower-case i, provides information on the keyboard shortcuts. 1.4 Technology Tips To succeed in this course, you will need a computer capable of installing and running Microsoft Word, Microsoft Excel, and jamovi. The Stout Surplus store (Facebook; website) may have laptops and desktops they are selling for cheap. You may also find it useful to have a mouse in this course since we’ll be pointing and clicking a lot in jamovi; you can get small portable ones to bring with you to campus if you’re in my in-person section. You may also find it beneficial to have two monitors in this course. When doing activities and assignments in this course, I recommend having jamovi open on one monitor and then the assignment and textbook open on the other monitor. The Stout Surplus store may have extra monitors they are selling for cheap. If you are unable to get a second monitor in this course, learn how to snap windows side by side in Windows or Mac. Learn also how to use the Alt+Tab function on your computer as well to switch back and forth from open applications. For Macs, use Cmd+Tab. When you are working on this course, I recommend that you only have open the application and windows you actually need for this course. Ample research has shown that multitasking does not work, and so when you switch from one activity to another (e.g., from working on another course to this one) you should wrap up your tasks, save your files, close them all, and then start fresh. This will clean up your taskbar and help you focus on this course. It will also free up your computer’s resources so it won’t start slowing down. Another friendly reminder: restart your computer frequently, at least once a week. Your computer will begin slowing down the longer you wait in between restarts. Also, be sure to update your computer regularly. Note that jamovi also has a Cloud version. The free version will not be sufficient for all this course, but you can pay for the cloud version if you wish. Please note that the desktop version is strongly recommended as it is free and should work fine with the minimum technical specifications of computers that Stout requires. 1.5 Errors, mistakes, and suggestions I am human, therefore I err. If you find an error in the textbook or something you think might be a mistake, please let me know ASAP so I can update this for everyone else. Let me know which section you find the error or mistake in and what the error or mistake is. For example, if there was an error here you could say, “There was an error in chapter 1, last section, that the first sentence should really be ‘To err is human (Alexander Pope, 1711).’” In addition, if you have ideas to help make this textbook even better, please let me know. I would love to make this a useful resource to you both during our course and in your future research. Help me in making that a reality! "],["statistics-foundations.html", "2. Statistics foundations", " 2. Statistics foundations You have learned about both quantitative and qualitative methods. We will be focusing primarily on quantitative methods in this class and in this textbook. By quantitative methods, I mean methods that predominantly collect data that deals with numbers. We can then analyze that data using statistical procedures, which we will shorthand to “statistics.” Understanding what we mean by statistics is the purpose of this chapter. "],["describing-our-data.html", "2.1 Describing our data", " 2.1 Describing our data First, let’s understand some basic statistics related to how we describe our data, including measures of central tendency (averages), measures of dispersion (spread), and measures of shape of the distribution (particularly a normal distribution). Here’s a video walking you through what we learn in this chapter. Measures of Central Tendency There are multiple measures of central tendency (these are all averages so you must be careful when you say that word to explain which type you mean!): Mean: the sum of all points divided by the total number of points. The mean can be susceptible to outliers. Median: the middlemost value. The median less susceptible to outliers and best used when the data is skewed. Mode: the most frequently occurring score. Our data can also be multimodal when there are multiple modes or bimodal when there are two modes. Note that depending on the shape of the distribution, the mean, median, and mode may not be the same value. If we have a normal distribution then they will be the exact same! However, if we have a positively skewed distribution, the mean and median will be pulled towards the positively skewed data, as shown in this figure by Peter Prevos. Measures of Dispersion There are also multiple measures of dispersion that describe the spread of our data: Range: the difference between the maximum and minimum value (e.g., if the minimum score is 17 and the maximum is 49, then the range is 32) Quartile: when a dataset is divided into four equal parts, the first quartile (Q1) is at the 25th percentile, the second quartile (Q2) is at the 50th percentile, and the third quartile (Q3) is at the 75th percentile. Note that the median is also the 50th percentile! Interquartile range: the middle 50% (Q1 to Q3) Variance: is the measurement of the spread between numbers in a dataset. More specifically, it is the sum of the squared deviations from the mean. This means first (a) calculating the mean, (b) subtracting each score from the mean (aka deviations from the mean), (c) squaring each of those deviations values, and (d) summing all those squared deviations. This is represented by the equation \\(\\frac{\\sum (X-\\mu)^2}{N}\\) Standard deviation: is the square of the variance. This is represented by the equation \\(\\sqrt{\\frac{\\sum (X-\\mu)^2}{N}}\\) if we are examining the whole population. If we only have a sample, we replace the denominator N with N-1. I sometimes present equations, like above, to help some folks better understand various things in this course. However, I very rarely expect you to calculate things, and when I do, I give you a lot of support in how to do so. Measures of the Shape of the Distribution We also have two main measures of shape that describe the shape of the distribution of our data: Skew: in a non-normal distribution, it is when one tail of the distribution is longer than another. Present in asymmetric distributions Negative skew: when the tail points to the negative end of the spectrum; in other words, most of the values are on the right side of the distribution Positive skew: when the tail points to the positive end of the spectrum; in other words, most of the values are on the left side of the distribution Kurtosis: the weight of the tails relative to a normal distribution. In other words, it’s how flat or skinny the distribution is. There are some fancy terms related to kurtosis that you may hear about, but honestly I don’t hear them used very frequently by researchers. Leptokurtic: light tails; values are more concentrated around the mean. Think of kangoroos which can be noted for “lepping”. Platykurtic: heavy tails; values are less concentrated around the mean. Think of a platypus! Student (a pseudonym of William Gosset) wrote this in his 1927 paper). When skew and kurtosis are zero, then we have a special type of distribution called the normal distribution in which the data are symmetrical on both sides of the mean. We sometimes call this a “bell curve”. Scribbr has a great article on the normal distribution I encourage you to read! Although there are many ways we can visualize the distribution the two most common ways we will visualize the distribution using jamovi are with a histogram which could also be a density curve or a boxplot. We’ll learn more about visualizing in Chapter 3 “Visualizing data”. "],["levels-of-measurement.html", "2.2 Levels of measurement", " 2.2 Levels of measurement This may or may not be refresher material for you, but it is extremely important you are familiar with the four levels of measurement. Categorical: variables that have categories to the levels, but cannot be analyzed with a mean because the levels are not proportionate. There are two types of categorical variables: Nominal: a categorical variable in which each level of the variable is named but there is no order to them (e.g., breeds of dogs). Nominal variables can only be analyzed with frequencies, modes, counts, or percentages. Nominal variables with only two levels (generally coded as 0 or 1, although jamovi can handle named levels), we call these special nominal variables as either binary, dummy-coded, or dichotomous nominal variables. For example, binary variables could be coded as yes or no or as absence or presence. Ordinal: a categorical variable in which each level of the variable is named and there is an order to them (e.g., ranks). We can analyze ordinal variables with frequencies and percentages, like nominal variables, but we can also analyze them using medians, minimum and maximum values, ranges, and percentiles. Continuous: variables with proportionate intervals between the levels, meaning they can be analyzed with a mean, SD, variance. There are two types of continuous variables (although for the purpose of this course we will simply call them continuous variables): Interval: a continuous variable that has intervals that are directly proportionate (e.g., the distance between 2-3 is the same as the distance between 5-6). We can analyze interval variables using means, variances, and standard deviations. Ratio: a continuous variable like an internal variable but can accommodate an absolute zero, meaning a zero is actually possible (e.g., weight, temperature in Kelvin, reaction time). We can also analyze ratio variables with means, variances, and standard deviations, but we can also conduct arithmetic operations like fractions and ratios. Note that in this class, I won’t ask you to differentiate between interval and ratio levels of measurement because the differences between the two are usually not meaningful in our work. You’ll even find that jamovi doesn’t differentiate between the two, either. Instead, you just need to know whether the variables are continuous, ordinal, or nominal. Examples of levels of measurement Confused still on the levels of measurement? Maybe this will help! One semester, a student asked, “Isn’t time a continuous variable?” To which I responded, “It depends on how it is measured!” If time was measured in a simple pre/post design, such as the start of the semester and the end of the semester, then it’s a nominal variable, and a specific type of nominal variable that we can call binary or dichotomous. If time was measured by month, January through December, then it would be ordinal because January is before (earlier than) February, for example. There is an order to the months, and a calendar that went March, October, April, January, etc. would make no sense. If time was measured as a continuous variable, it could be something like the exact day, exact time, response time, or time remaining. Here’s another example. Notice that studying can be measured at different levels. Depending on the nature of the question and response options, it might be nominal, ordinal, or continuous! Here’s an example of data at the continuous, ordinal, and nominal level. Name Study_Continuous Study_Ordinal Study_Nominal Name (Character) Hours studied per day Likert scale of amount of studying Whether or not they study every day Jesus 5.0 A great deal Yes Nicky 4.5 A great deal Yes Bradford 3.2 A moderate amount Yes Sylvia 1.7 A small amount Yes Martha 0.2 Rarely Yes Lillian 0.0 Never No Trayvon 0.0 Never No We can make any continuous variable into an ordinal and nominal variable and any ordinal variable into a nominal variable. But if we have a nominal variable we cannot make it ordinal, nor can we make an ordinal variable continuous. In other words, continuous variables contain more information than categorical variables. Often, we want to avoid losing information and so we should aim to keep the variable at the highest level of measurement. Because continuous variables have more information, we want to avoid doing things like mean or median splits or “collapsing” categories when you can. A mean or median split, which involves finding the mean or median value and splitting the data so it’s above the mean/median or below the mean/median, is making a continuous variable as nominal, which is removing information. Collapsing categories may also further reduce the information. Another thing to keep in mind is that just because we put numbers to something does not necessarily make it continuous! Be careful and think critically. If I said cat = 1, dog = 2, and frog = 3 it doesn’t make it an ordinal or continuous variable. "],["descriptive-vs-inferential-statistics.html", "2.3 Descriptive vs inferential statistics", " 2.3 Descriptive vs inferential statistics There are basically two different types of statistics: Descriptive statistics are used to summarize, organize, and overall describe our sample data. Typically, we do so using measures of central tendency (e.g., mean, median, mode), measures of dispersion (e.g., range, standard deviation, variance), and shape (e.g., skew, kurtosis). We may also visualize the data using tables or graphs. Inferential statistics are what we use when we collect data about a sample and see how well that sample infers things about the population from which the sample comes from. Typically, we do so with statistical tests like the t-test, ANOVA, correlation, chi-square, regression, and more. We can visualize the relationship between the population, sample, descriptive statistics, and inferential statistics (see figure below). We are typically interested in a population of interest but may not be able to collect data from the entire population because of budget, time, access, or other constraints. Therefore, we typically sample from the population; ideally, we do so randomly, but there are other types of sampling methods available that are covered in research methods. We then use descriptive statistics to describe our sample data, and we and inferential statistics to make generalizations about the population from which they were selected. Note that we typically use both descriptive and inferential statistics. However, some studies may be purely descriptive with no inference back to the population. We typically always describe the sample when we are performing inferential statistics though. An example This has been pretty abstract so far. Let’s go through a fairly simple research study to walk through all of this. Imagine we’re conducting an experimental study examining whether watching Schitt’s Creek–a very good show–versus watching video lessons on studying techniques–useful, but boring–improved test performance in UW-Stout students. Our population of interest is therefore all UW-Stout students, roughly 9,500 students total. We cannot include them all in our study; it wouldn’t be feasible for us to collect all that data and probably not possible to get the university to get on board with the study of the entire student body. Therefore, we smartly decide to only collect data from a sample of the student body. Who might our sample be? Ideally, we’d gather a random sample of the 9,500 students. However, to do that we’d likely need to still get university approval and get a list of a portion of student emails for recruitment purposes (oversampling because our response rate is unlikely to be 100%). I just want to do this study to show what descriptive and inferential statistics are, so I just use students in my two sections of introduction to psychology classes (around 80 students total) as my population. This is definitely not a random sample, but a fine study for our illustrative purposes. We conduct our study–let’s assume we’re fabulous researchers and it worked out perfectly. We randomly assign half our students to watch Schitt’s Creek as part of their studying, and the other half watch video lessons on studying techniques. They have an exam a week later and we measure their accuracy on that exam. We then want to know: which group performed better on the exam? First, let’s describe the sample. We would likely visualize our results, perhaps as a histogram of all test scores, maybe separated by which group they were in. This would help us look at whether our data is normally distributed (more on this in a subsequent chapter on assumption checking). We would get the descriptive statistics: probably the mean, maybe the median if our data is skewed, the standard deviation and variance, and the range. If we wrote up our results and didn’t share a visualization, this information would give a good sense of our data to our readers. But what we really want to know is: which group performed better on the test? For that, we need our mean, standard deviations, and sample sizes for both groups. We then plug the numbers into the equation for this particular inferential statistic (in this case, an independent t-test, but we’ll learn about that later) or–even better–we perform the statistic in our statistical software (jamovi). It spits out our statistical value and our p-value and we can then infer what the results mean for our population and answers our research question.1 You might be wondering: well, what were the results? Which group performed better? As much as I love Schitt’s Creek, most students don’t know how to study well, and so the students who watched the video lessons on studying techniques far outperformed the students who watched Schitt’s Creek. Interested in better techniques for studying? Check out The Learning Scientists. This article does a good job of summarizing the research on effective study practices.↩︎ "],["design-methods-key-terms.html", "2.4 Design &amp; Methods Key Terms", " 2.4 Design &amp; Methods Key Terms The following covers some terminology that will be helpful to keep in mind throughout the semester. Variables We tend to talk about two different types of variables in our studies: Independent variable (IV; also known as the predictor variable or exposure variable): this is the variable that is thought to be the cause of some effect. In experimental research, it is the variable that is manipulated. Dependent variable (DV; also known as the outcome variable): this is the variable that is thought to be affected by changes in the IV. In research, it is the variable that is measured. There are other types of variables we may be interested in. Note that these variables are not considered independent variables because we would not manipulate them or try to examine how they cause the dependent variable. Confounding variable: a variable that affects or is related to both the independent and dependent variable Covariate: a variable that only affects or is only related to the dependent variable Study design terms Some terms you should be familiar with: Between-group/subject design: in a study, participants are only exposed to a single condition. In other words, there are different people in each condition. Correlational research: a study in which causality cannot be claimed; correlation does not infer causation! It is, however, one of three necessary conditions to infer causality. It is a necessary but insufficient alone condition. Cross-sectional research: also called non-experimental research; the IV is not manipulated and there is no random assignment. Furthermore, data is only collected at one time point (as opposed to longitudinal research) Experimental research: the IV is manipulated and there is random assignment Falsification: A key way we separate science from pseudo-science is that we attempt to falsify our hypotheses as opposed to verify our hypotheses. Null hypothesis significance testing (NHST) is about falsifying the null hypothesis; we can never truly verify our alternative hypothesis. Hypothesis: What we think the answer to our research question is (often our alternative hypothesis). The alternative and null hypotheses must be mutually exclusive (a result can’t satisfy both) and exhaustive (all possible results are specified) Alternative hypothesis: Often that the IV had an effect on the DV; can be specified as a two-tailed (an effect) or one-tailed (greater/less than) hypothesis Null hypothesis: The null hypothesis is the opposite of the alternative hypothesis and encompasses all the rest of the possibilities. A special type of null hypothesis is the nill hypothesis which is that there is no effect on the DV. Qualitative methods: Broadly, methods that focus on words and meaning (e.g., interviews) Quantitative methods: Broadly, methods that focus on numbers and statistics (e.g., Likert scales) Quasi-experimental research: the IV is manipulated but there is no random assignment Randomization: participants are randomly assigned to conditions Repeated-measures design: participants are repeatedly measured on the dependent variable, either across conditions or across time Theory: A description of a behavior that makes predictions about future behaviors Variation: Systematic: researcher introduces error systematically into the study, especially into one condition over another. For example, by randomly assigning participants into one of two conditions, we are introducing systematic variability between participants. However, it could be unintentional systematic variation; for example, perhaps we have two researchers collecting data and one is mean and the other is nice, and so participants respond differently depending on which researcher collects data from them. Unsystematic: random variation Within-group/subject design: in a study, participants are exposed to every condition. In other words, there are the same people in each condition. Reliability and validity Reliability: the consistency of a measure by time (test-retest reliability), across items (internal consistency) or across different researchers (inter-rater reliability). Check out the chapter “Reliability” for more information on measuring reliability. Validity: the extent to which a test measures what it claims to measure Construct validity: validity of inferences about the higher order constructs that represent sampling particulars. There are multiple types of construct validity; here are a few: Content validity: experts using their judgment that something measures what it is supposed to measure Convergent validity: correlations among two theoretically related constructs (or measurements) are strong and positive Divergent validity: correlations among two theoretically not-related constructs (or measurements) are zero/null Criterion validity: content on one test (predictor) correlates with performance on relevant criterion measures (outcome) Statistical validity: validity of inferences about the correlation between treatment and outcome Internal validity: validity about whether the observed relationship between A and B reflects a causal relationship between A and B External validity: validity of inferences about whether the cause-effect relationship holds over variation in persons, places, treatment variables, and measurement variables Statistical terms Degrees of freedom (df): Degrees of freedom are the number of values that are free to vary and are not constrained. The easiest way to understand them is with an example of the mean. If you know the sum of 4 numbers, then 3 of those numbers are free to vary but the fifth value is dependent on (and constrained) to the values of the other. If your sum is 10, you could have three random values of 2, 3, and 1 (which are free to vary) but your fourth number must be 4. Therefore, the degrees of freedom in this example would be 3, which is n - 1. The statistical tests all have different ways of calculating degrees of freedom. For example, the one-sample t-test is n - 1, the independent t-test is n1 + n2 - 2, and the dependent t-test is n - 1 where n is the number of pairs. Confidence intervals: A confidence interval is such that in repeated sampling (e.g., replicating a study over and over and over again) then 95% of those confidence intervals should contain the true population parameter. You can see this visualized, and learn more about it, with this great visualization and read this great blog post. You can also read a chapter on this topic by Daniel Lakens. Other terms If other terms come up in the course of the semester that you believe should belong in this key term website, include it in your weekly reflection so I can update this page! "],["overview-of-jamovi.html", "3. Overview of jamovi", " 3. Overview of jamovi jamovi is a free and open statistical software that helps us run our descriptive and inferential statistics. Why are we using jamovi and not another program? Did I mention it’s free? You won’t ever have to pay a dime to use the software in the future, unless you use the paid version of jamovi Cloud. It’s open source, meaning that the statistical community helps support and improve the program. As jamovi says, “jamovi is made by the scientific community, for the scientific community.” It’s built on top of the R statistical language, meaning you can begin learning how to code (if you want). I do all of my statistical analyses using R in a different program called RStudio (actually this book was developed in RStudio and hosted on GitHub!). It’s a very powerful tool which is also free and open source. It’s incredibly easy to learn and use. I have taught statistics using both SPSS and jamovi, and students greatly prefer jamovi. It promotes reproducibility. jamovi will save your data, analyses, options, and results all in one file so you can easily pick up where you left off. This will make your homework and future data analyses a breeze. Installing jamovi To install jamovi, go to https://www.jamovi.org/download.html. I recommend downloading the “current” version with the latest features, but you may want to get the “solid” version that is more stable. jamovi has more documentation on how to install jamovi per operating system. Installation on macOS and Windows should be pretty straightforward. Here’s some guidance if you are getting an error on Windows that “The app you’re trying to install isn’t a Microsoft verified app.” Chromebooks may have an issue installing jamovi if your version of Chromebook does not use Linux. Watch this video for support on installing jamovi on a Chromebook. Navigating the jamovi interface I recommend you watch this video by Alexander Swan on navigating the jamovi interface in jamovi to understand everything. Note that he is using a Mac in his demonstrations and that he has downloaded a ton of add-on modules which is why his Analyses tab looks different from yours. Opening data in jamovi I recommend you watch this video by Alexander Swan on opening files in jamovi. Opening SPSS (.sav) files in jamovi Here’s another video walking you through how to open .sav files in jamovi. Entering data in jamovi Occasionally you need to manually enter data into jamovi instead of opening a pre-existing data file. I recommend you watch this video by Alexander Swan on entering data manually in jamovi. Annotating output in jamovi Sometimes I may ask you to submit a file of your output with annotations. I recommend you watch this video by Alexander Swan on annotating output in jamovi. Additional jamovi videos and resources Here are some additional resources on jamovi I recommend you bookmark and use when you are struggling or need additional support: jamovi 2022 tutorials by Alexander Swan on YouTube jamovi docs provide more support for using jamovi, including tips for transitioning from SPSS to jamovi and from jamovi to R caution: these use an older version of jamovi! Introduction to jamovi LinkedIn Learning course by Barton Poulson, founder of datalab.cc (you can find the video on YouTube and other places now, too, if you do not have LinkedIn Learning). "],["describing-data.html", "4. Describing data", " 4. Describing data As a reminder, descriptive statistics are used to summarize, organize, and overall describe our sample data. "],["data-variables.html", "4.1 Data variables", " 4.1 Data variables First, it’s important to understand the different types of variables in jamovi and how they map onto our levels of measurement. Variables in jamovi can be one of three data types: Integer, meaning the values are discrete whole numbers Decimal, meaning the values are numbers with decimals Text, meaning the values are alphanumeric, not just numeric Furthermore, variables in jamovi can be one of four measure types: Nominal Ordinal Continuous (meaning jamovi combines interval and ratio and doesn’t distinguish between the two) ID (used for any identifying variable you likely wouldn’t ever analyze, like participant ID number or name) There are a few great things about jamovi when it comes to these data variables. First, jamovi will try to automatically determine what the data and measure types are when you type in data or when you open a dataset; this is fabulous, until it goes wrong. It’s important that you always double check your data and measure types first! Second, those little icons will be really helpful to let you know what variables can go in which boxes. For example, we would never analyze a nominal variable as our dependent variable for a t-test, and jamovi will help remind you of that. When performing an independent samples t-test, the dependent variables box will have a little ruler icon indicating you should be putting continuous variables in that box. Similarly, it will tell you to put nominal or ordinal variables in the grouping variable (independent variable) box. Sweet! "],["describing-your-data.html", "4.2 Describing your data", " 4.2 Describing your data We explore our data partly to describe our data and partly to check our data before performing inferential statistics. jamovi puts all our descriptive statistics into one useful analysis under the Analyses tab within the Exploration menu called Descriptives. Describing nominal or ordinal data Nominal and ordinal data is described mainly using frequencies. Choose your categorical variable(s) and move it to the Variables box and then select the option Frequency tables. Note that it tells you it’s best for nominal and ordinal data with the two icons! I recommend you watch this video by Alexander Swan on how to describe categorical data in jamovi. Describing continuous data Continuous data is described using a variety of measures of central tendency, dispersion, and more. Choose your continuous variable(s) in the list of variables on the left side and move it to the Variables box. I recommend you watch this video by Alexander Swan on how to describe continuous data in jamovi. In the Descriptives analysis (these are under the Statistics drop-down menu, there are a ton of possible options! Sample size: you can ask for the sample size (N) and number of missing values (Missing) Percentile values: these are useful for creating quartiles (Cut points for 4 equal groups) or Percentiles of various sizes. Dispersion: you should already be familiar with most of the measures of dispersion, particularly the Minimum and Maximum, but there is also the Std. deviation (SD) and Variance (which is just SD2). We’ll learn about the S. E. Mean later. Central tendency: similarly, you should also be familiar with all of the measures of central tendency: Mean, Median, Mode, and Sum. Distribution: you should also be familiar with both Skewness and Kurtosis and later we will learn what those values mean and how that helps us test for normality. Normality: lastly, there is a statistical test for normality called the Shapiro-Wilk test that we will learn about later. Describing one variable split by another variable Sometimes we want to get the descriptive statistics for one variable across multiple groups of another variable. Usually, this is a continuous variable split by a categorical variable. In that case, move your continuous variable to the Varables box and the variable you want split by categories in to the Split by box. "],["writing-up-descriptive-statistics.html", "4.3 Writing up descriptive statistics", " 4.3 Writing up descriptive statistics We’ll learn more about writing up our inferential statistics results later, but first let’s learn how we might report our descriptive statistics. In small examples, we might write-up our descriptive statistics into a paragraph2 (note: I also describe an independent t-test and a chi-square test of independence in this paragraph): In examples with many variables, we might write-up our descriptive statistics into a table3: This comes from Wanzer (2017) Developmentally appropriate evaluations: How evaluation practices differ across age of participants↩︎ This comes from Wanzer et al. (2020) Experiencing flow while viewing art: Development of the aesthetic experience questionnaire↩︎ "],["visualizing-data.html", "5. Visualizing data", " 5. Visualizing data “A picture is worth a thousand words,” and in a world in which journal articles have word count limits, figures and graphs are priceless. They are also an incredibly powerful way to examine your data because it can often illuminate patterns you may not be able to see through a table. Remember: it is incredibly important to always visualize your data! You never know what descriptive statistics may be hiding. Here’s a video walking through why it’s so important to look at your data. jamovi has some plots built into its platform, both under the Plots drop-down menu in the Descriptives analysis and as options for many of the inferential statistical analyses. We’ll learn more about how to choose and conduct better data visualizations later, but for now here are some recommended visualizations depending on what you are trying to do. Note that we will do most of our visualizations in jamovi, but we may also learn how to visualize data via Excel. There are also excellent LinkedIn Learning courses on data visualization in Excel and other tools that I strongly recommend; they are free for you if you are a UW-Stout student. "],["a-continuous-variable.html", "5.1 A continuous variable", " 5.1 A continuous variable First, there are two Histogram options: Histogram and Density. These are useful for seeing the overall distribution of your data and to help check for normality. Which should you use? I think they’re both pretty great, and in fact you can combine the two to have a histogram plot with a density overlay. I like this option best because it presents more information and better lets us see if the if the density curve looks normally distributed. "],["a-categorical-variable.html", "5.2 A categorical variable", " 5.2 A categorical variable For this you would choose the single option under Bar Plots: Bar plot. It will simply show the frequencies of a categorical variable. "],["a-continuous-variable-split-by-a-categorical-variable.html", "5.3 A continuous variable split by a categorical variable", " 5.3 A continuous variable split by a categorical variable There are three options under Box Plots: Box plot, Violin (which is really a density plot with its mirror image!), Data (which can be Jittered or Stacked; I prefer Jittered so you can see the density of data points really well), and Mean. Personally, I love checking all four boxes! This gives you the best of all of them: the distribution of your data with the Violin option, the quartiles and mean with the Box plot option, a visualization of all your data points using the Data option, which is really useful because the other two options can be hiding weird things in your data, and what the Mean is. "],["expanding-your-data-visualization.html", "5.4 Expanding your data visualization", " 5.4 Expanding your data visualization Although these can be useful plots, I often do most of my data visualizations in other platforms. For most of my work, I use Excel because I find it pretty easy to make beautiful graphs. Here’s an example of a visualization I made in Excel4: For some more complicated figures, I turn to the ggplot2 package in R. Here’s an example of a visualization I made in R5: This comes from Wanzer et al. (2020) Promoting intentions to persist in computing: An examination of six years of the EarSketch program↩︎ This comes from Wanzer (2020) What is evaluation? Perspectives of how evaluation differs (or not) from research↩︎ "],["cleaning-data.html", "6. Cleaning data", " 6. Cleaning data There are four basic types of cleaning we will be learning about: checking your data is setup correctly, computing new variables, transforming variables, and using filters. The following video walks through some of these data cleaning techniques. "],["data-setup.html", "6.1 Data setup", " 6.1 Data setup As previously mentioned, it’s really important to check that the data types and measurement types of your variables are correct. You should open the Setup () option under the Data tab to check. When you’re in Setup, here’s the things you should be doing for all variables: Make sure the variable name is meaningful to you. You may also want to change it to something that will appear nicely in your data visualizations or tables (e.g., don’t write Q35 but rather BDI_Score). Add a description to your variable so you have more context. Maybe you write Average score of all BDI items for the description of your BDI_Score variable. Check your measurement level (i.e., nominal, ordinal, continuous, ID) and data type (i.e., integer, decimal, text) are correct. Specify if there is a code for missing values. Make sure the code does not match the code you use for actual variables! For example, if I have a variable that ranges from 0-10, then I wouldn’t use 9 as a code for missing values; instead, I might use 99 or -9. Add labels to the categories in categorical variables. For example, the variable Athlete codes 0 for non-athlete and 1 for athlete. Rather than keeping just the 0 and 1, you can specify under Levels that 0 is non-athlete and 1 is athlete. "],["compute.html", "6.2 Compute", " 6.2 Compute Sometimes you need to create new variables from your raw (meaning uncleaned) data. I recommend you watch this video by Alexander Swan on computing variables in jamovi. Perhaps you collected data on a scale that has five items. Normally, we create an average score of all the five items and that new computed average score is what we use in our analyses. Let’s open the Big 5 dataset built into jamovi. You can open this dataset by clicking the three horizontal lines on the top left of jamovi (the menu), choose Open, then select Data Library. In the main Data Library folder is a dataset called Big 5. This dataset has the scores on all five subscales of the Big Five personality test. Let’s imagine we want the average score of the entire Big Five test. Go to the Data tab in jamovi. Click Compute. Note that jamovi will create a new variable directly after the column you have selected. If you want this variable to be at the end of your dataset, you should select the last column of the dataset and then click Compute. Rename the computed variable to something meaningful (e.g., Big5_Avg) Add in an informative description for your computed variable (e.g., Mean score of the Big 5 personality traits) Click the \\(f_x\\) (formula) button. Under Functions, find the function we are computing. In this case it is Mean but sometimes scales want us to use Sum. These are the two functions we will primarily use in this course. jamovi tells us more information about the particular formula function. In this case, it says MEAN(*number1, number2, ..., ignore_missing=0, min_valid=0). Therefore, we need to specify the numbers we are calculating the mean for. The ignore_missing argument is defaulted to 0 (do not ignore missing values) but you could set it to 1 (ignore missing values). The min_valid argument is defaulted to 0 variables, but you could specify the minimum number of valid responses we want in order to calculate the function. Double click on the function Mean to enter it into the formula box. You should see =MEAN() now. Click inside the parentheses and double click the first variable of the Big 5. Then add a comma. Double click the second variable and add a comma. Repeat until you have all variables in the formula box. When you are complete, click outside of the function box and it will automatically create your computed variable. jamovi will often tell you if there is an error in your formula. For example, if you get the warning “Column NAME does not exist in the dataset” that means you entered something incorrect; typically, you either mispelled a variable or forgot to separate variables with commas. You can see what we need to do with this dataset below. There’s actually no missing data, so the two additional arguments aren’t necessary for us to worry about. Note that this was creating an average score using the MEAN() function. Sometimes, psychological scales want you to create a total score using the SUM() function. jamovi also has a lot of other functions you may need to use in the future. After computing a new variable, I always like to double check the work to make sure it’s okay. I review to make sure if there is any missing data that it’s appropriate (e.g., I specified min_valid=9 for a 10-item scale, so only people with at least 9 valid responses should have a mean calculated). If I’m doing other basic math, I also like to make sure that I specified it correctly by manually calculating a few rows myself. If you’d like to learn more about computed variables in jamovi, check out this jamovi blog post on the topic. "],["transform.html", "6.3 Transform", " 6.3 Transform Sometimes we want to take an existing variable and transform it in some way or we want to do a computation across multiple variables (e.g., reverse-score multiple items in a dataset). I recommend you watch this video by Alexander Swan on transforming variables in jamovi. If you want to learn more about transforming variables, the jamovi blog has a great blog post on the topic. Recoding Sometimes variables need to be recoded. As one example, if you have a text variable with many options, sometimes you want to recode things into a new variable with fewer options. This might mean taking something with 9 categories and collapsing them down to 3 overarching categories. The benefit of transforming through recoding is that you retain the original variable with the 9 categories but you also now have the new variable with 3 categories. Let’s return to the Big5 dataset and recode the Neuroticism scale into low, moderate, and high neuroticism. The scale ranges from 1-5, so I’m going to say that scores between 1-2.333 are low, 2.334 to 3.666 is moderate, and 3.667 to 5 is high. Go to the Data tab in jamovi and select the variable that you want to be transformed in the dataset. In this case, ensure Neuroticism is selected. Click Transform. Rename the transformed variable to something meaningful. I typically try to write it in such a way that it retains the old variable name. For example, we can transform the Neuroticism variable into the newly transformed variable Neuroticism_cat where “cat” stands for category. Ensure the correct Source variable is being used. The source variable is the variable that is being transformed; in this case it is Neuroticism. Under using transform, select Create New Transform... from the drop-down menu. If you have already created the transformation (e.g., you are applying the same transformation to multiple variables) then you can select the transformation from the drop-down menu instead. Here’s what it should look like prior to creating a new transformation: Rename the transformation to something meaningful. In this case we are categorizing into three categories so I might rename it Categorize into 3 categories Add a description if you desire. Specify the Variable suffix. This is what renames your transformed variable from something really annoying (e.g., Neuroticism - Categorize into 3 categories) into something succinct (e.g., Neuroticism_cat). In this case, the suffix would be _cat. This is particularly helpful if you are transforming multiple variables at once so they don’t all have really long names. Select Add recode condition for n-1 recodings you are doing. In this case, we have 3 categories so we only need to specify 2 (3-1=2). Your view should look something like this now: We now need to specify the recode formulas. First, after the if $source statement we enter &lt; 2.334 and then in the use box we enter 'low'. Note the usage of apostrophes (you can also use quotation marks) which tells jamovi this will be text data. If you don’t wrap things in apostrophes and quotation marks, it will assume it is numeric information (of which you should be putting numbers containing 0-9) For the second if $source statement we enter &lt;3.667 and then specify to use 'mod'. For the else use statement at the end, we enter 'high' because we don’t need to specify anything further. If it’s not &lt;2.334 and not &lt;3.667 then its &gt;=3.667 which is all that’s left in this particular data. Specify the correct Measure type. jamovi automatically guessed that the data was ordinal, but I like to specify it just to be sure. Enter ordinal for our particular data since low &lt; mod &lt; high. Your data should now look like this: Just like with computing new variables, when transforming new variables I like to peruse the newly transformed variable to make sure it did what I expected. In this case I see that participant 35 has a Neuroticism score of 1.917 which was correctly coded as low whereas participant 40 has a Neuroticism score of 3.688 which was correctly coded as high. A quick perusall makes me feel confident that I did my transformation correctly! Reverse-scoring Sometimes items need to be reverse-scored because the items are in the opposite direction of the rest of the items in the particular scale or subscale. Let’s imagine we have a Happiness Scale with the following four items: I am happy. I am content. Life is overall positive. I am unhappy. The happiness scale suggests that higher scores is higher happiness. However, the fourth item is opposite such that a person scoring higher on that item actually indicate lower happiness. Therefore, we would need to transform that item to reverse-score it such that it’s in the same direction of all the items. The way we do this is by recoding the levels so the highest score is the lowest score and so on. For example, if it were rated on a 5-point scale then you would need to recode so a 1 = 5, 2 = 4, 3 = 3, 4 = 2, and 5 = 1. Alternatively, instead of transforming you could use compute where the formula is the maximum value + 1 minus the value. For example, if you have a 5-point scale then you would do “6-variablename” in the computation box. We can use the same recoding feature as before to do this. The only difference is that we are going to use the double equal sign == (R is a bit weird in that we use == to mean =). See the screenshot below to see how we are specifying that if the source variable equals the value, then use the reverse-scored value. We should add 4 recode conditions in this case and then have the final else use value for the 5th category. Multiple transformations Sometimes we want to do a transformation across multiple variables. Perhaps we have multiple items that need to be reverse-scored. Or maybe like in our first example above we want to use our previous Low_mod_high transformation to perform on all the subscales of the Big 5. We can click a new variable (e.g., Openness), select Transform, rename the variable, and select the Categorize into 3 categories transformation we already used. Voila! The work we did previously can easily be used again in this analysis. Alternatively, you can select all the variables at once that you want to transform and then specify the same transformation to be used across all four variables. "],["filter.html", "6.4 Filter", " 6.4 Filter Sometimes we only want to analyze certain pieces of our data. We can filter by rows and by columns. Check out this blog post by jamovi on more details of filters. Row filters Let’s imagine we only want to analyze data from people who are low in neuroticism. We would create the following filter: You’ll notice in the dataset it will add a new column named Filter 1 (the name of the filter) and there will either be an X or a green check mark indicating whether it’s removed (X) or kept (check) in the analyses. If you want to take off the filter, but keep it available, click on the filter column and toggle the green button on the top right from active to inactive. It will then grey out the column, meaning it’s not being used. A couple things to note: Commonly, we want to specify the variable equals something. You would use the double equal sign: == Another common thing you may want to specify is that the variable is not equal to something. You would use the following: != Otherwise you should be familiar with the other operations from previous math courses: &lt;, &gt;, &lt;=, &gt;= Column filters Column filters are useful when you want to use a filter for some but not all of your analyses. Rather than creating a filter, we need to compute a new variable using the FILTER() function. For example, we learned how to reverse score our hypothetical happiness item above. We could then say we only want people who are high in that variable for another analysis. We could apply a column filter that is FILTER(Scale1_Reverse_Scored, Scale1_Reverse_Scored &gt; 3) . "],["hypothesis-testing.html", "7. Hypothesis testing", " 7. Hypothesis testing Now that we’ve covered descriptive statistics and are familiar with our statistical software, it’s time to turn to inferential statistics. Remember, we conduct inferential statistics because we often cannot collect data from an entire population. Therefore, we collect a sample to draw inferences about the population of interest. One of the ways we make inferences is using hypothesis testing. We are going to be learning about Null Hypothesis Significance Testing (NHST), which means we test and make inferences about the null hypothesis (which we’ll learn about in more detail soon). I have created a 4-page “cheat sheet” of the hypothesis testing 4-step process detailed here, with brief details on how it differs for the various inferential statistics we’ll learn in this textbook. I highly recommend you save this and refer to it often. If possible, print it out and have it by your side as you go through all of your inferential statistics so you know how to go through each step! Statistics with jamovi - Overarching handout Note: It will not preview the file in GitHub. When you click the link, click the download button to download the raw file. Regardless of the inferential statistic we are performing, hypothesis testing goes through the same basic set of procedures: Look at the data by examining the descriptive statistics and describing your hypotheses. Check assumptions to ensure your data is satisfactory for performing the inferential statistic (or choosing the correct statistic depending on which assumptions are met). Note that this is covered in Chapter 6 so we won’t discuss it in this chapter. Perform the test by running the inferential statistic by hand or in jamovi. Interpret the results and make a decision about whether you reject or fail to reject the null hypothesis, write-up the results in APA format, and provide a visualization of the results. Let’s go through each of these in turn, using a hypothetical example. "],["example-of-hypothesis-testing.html", "7.1 Example of hypothesis testing", " 7.1 Example of hypothesis testing Imagine a researcher wants to replicate Alburt Bandura’s famous Bobo doll experiment. In their replication study, the researcher randomly assigns 30 six-year-old children to one of two conditions: one group watches a video of an adult showing aggressive behavior toward a Bobo doll and the other group watches a video of an adult passively playing with a Bobo doll. After watching their assigned video, children then went to the same room from the videos with the same Bobo doll. Researchers observed for aggressive behaviors6. The example we are going to go through is an example of an independent samples t-test. There are two groups (children watching aggresive behaviors vs children watching passive behaviors) being measured on their aggressive behaviors. Although I do not expect you to understand an independent samples t-test to get through this chapter, you may want to explore that chapter in the textbook as well. Otherwise, we will learn about it in more detail later. Step 1. Look at the data The first step is to look at our data. We need to make sure we understand our study, what research design was used, how data was collected, and what the dataset looks like. Describe the data Although we are working with a hypothetical scenario, we can think about what the data would look like in a spreadsheet. We have two variables: The independent variable is what condition of the study they were randomly assigned to. The variable might be called “condition” which might have two categories or levels, one being “aggressive” and the other being “passive”. The dependent variable is the observed aggressive behaviors. The variable might be called “aggression” which would be a continuous measure of how many aggressive behaviors were observed for the child. This would result in two columns, one for each variable, and 30 rows, one for each participant in the study. Specify the hypotheses As mentioned previously, we are using Null Hypothesis Significance Testing, or NHST. We are therefore testing hypotheses in our inferential statistics, and specifically we are testing the null hypothesis. We need to first write out our hypotheses, which are our alternative and null hypotheses. The alternative hypothesis is that there is an effect of the IV on the DV, or a relationship between two variables, or difference between groups. We usually write this out as H1 although sometimes you’ll see it written as Ha. Your hypotheses should very clearly describe both the IV and DV. The null hypothesis is that there is no effect, no relationship, or no difference. Most null hypotheses tend to be nill hypotheses, and nill means zero. That’s hopefully an easy way to remember the difference between null and alternative hypotheses. We usually write the null hypothesis out as H0. Regardless of our research question, our null hypothesis is always that there is no effect. This is what “null” means: none or zero effect. The two hypotheses—our alternative and null hypotheses—must be mutually exclusive and exhaustive. Mutually exclusive means a potential result of the study cannot support both the alternative and null hypothesis; it must exclusively support only one. Exhaustive means the entire possible universe of results must be captured in our two hypotheses; it must exhaust all possible results. A common error students make in writing out their hypotheses is that they are not mutually exclusive or exhaustive. If there is any possible value that, if you got it, would not support either hypothesis then your hypotheses are not exhaustive. If there is any possible value that, if you got it, would support both hypotheses then your hypotheses are not mutually exclusive. We might also have directional or non-directional hypotheses. Directional hypotheses are also called one-tailed hypotheses because only one tail of the distribution would lead us to fail to reject the null hypothesis. Non-directional hypotheses are such that we don’t know whether the difference will be greater or less than 0, but we just think there will be a difference; these are also called two-tailed hypotheses because both tails of the distribution would lead us to fail to reject the null hypothesis. This will make a little more sense below and a lot more sense in the next chapter. Let’s go back to our Bobo doll replication study. What might the hypotheses be? There should be theory and research to support alternative hypotheses. There is ample research now that viewing aggression leads to aggression through imitation and observed learning. Therefore, the researcher likely has a hypothesis that watching the aggressive adult will lead to more aggressive behavior than watching the passive adult. Therefore, our hypotheses would be: H1: Children watching the video with the adult aggressively playing with the Bobo doll will exhibit more aggressive behaviors than children watching the video with the adult playing passively. H0: There will be no difference in children’s aggressive behaviors between the two groups OR children watching the video with the adult aggressively playing with the Bobo doll will exhibit fewer aggressive behaviors than children watching the video with the adult playing passively. Let’s analyze these hypotheses in more detail: The hypotheses are mutually exclusive because no possible result could be both more aggressive AND no difference in aggression, for example. The hypotheses are exhaustive because all possible results are that there are fewer, zero, or more aggressive behaviors when comparing the groups. Basically, it encompasses negative infinity to positive infinity! The hypotheses are one-tailed because we anticipate a direction in the difference between the groups. If we had no prior thought about how one group would compare to another, then we could have had a two-tailed, non-directional hypothesis. Furthermore, note how I have clearly described the IV (the groups of watching the video with the adult aggressively vs passively playing with the Bobo doll) and the DV (aggressive behaviors). A common error in a directional hypothesis like this is to forget that the null hypothesis is both no difference and the opposite. In other words, we have three possible options for our null and alternative hypotheses based on direction (\\(\\mu\\) is the Greek letter “mu” and we often use it to signify the mean): Two-tailed One-tailed (greater) One-tailed (less than) Alternative (H1) \\(\\mu_1\\) != \\(\\mu_2\\) \\(\\mu_1\\) &gt; \\(\\mu_2\\) \\(\\mu_1\\) &lt; \\(\\mu_2\\) Null (H0) \\(\\mu_1\\) == \\(\\mu_2\\) \\(\\mu_1\\) &lt;= \\(\\mu_2\\) \\(\\mu_1\\) &gt;= \\(\\mu_2\\) Since we’re talking about mean differences, we could also reformulate the above table slightly differently: Two-tailed One-tailed (greater) One-tailed (less than) Alternative (H1) \\(\\mu_{diff}\\) != 0 \\(\\mu_{diff}\\) &gt; 0 \\(\\mu_{diff}\\) &lt; 0 Null (H0) \\(\\mu_{diff}\\) == 0 \\(\\mu_{diff}\\) &lt;= 0 \\(\\mu_{diff}\\) &gt;= 0 Note: != means “not equal” like the ≠ symbol. I write != because that is the notation that R uses for “not equal.” Similarly, you might be wondering why I use == instead of just =. Again, this is the notion R uses for “exactly equal to.” In R, a single equal sign is usually equivalent to the assignment operator (e.g., x = 10 means assign 10 to the variable x). Step 2. Check assumptions We’re going to skip over this for now (read more in the “Parametric Assumptions” chapter later). Just keep in the back of your head that you need to understand whether you met the assumptions to know exactly what statistical test to perform. Step 3. Perform the test For this example chapter, we’re going to walk through how we might do these analyses by hand. Back in the day before computers existed, or before they were more widely available, this is how folks would analyze their data! This helps us both understand the “behind the scenes” processes of our statistical programs, but also gives us a greater appreciation for what our statistical programs provide us. The basic thing we’re trying to do with our inferential testing is determine what constitutes “more” aggression versus “no difference” in aggression. No difference seems easy. That’s a difference of zero, right? Well, not exactly, because it’s highly unlikely we would get an exact difference of zero. Therefore the question is: which values are close enough to a difference of zero that we’d still say that there is no difference? If our values are within that range, then we would fail to reject the null hypothesis. If our values are outside that range, then we would reject the null hypothesis. Note my language carefully here: fail to reject the null hypothesis OR reject the null hypothesis. Note how I am not saying support the alternative hypothesis! Through NHST, we are only ever testing the null hypothesis and therefore can only make conclusions about the null hypothesis. This is why we need replication studies to provide ample support for alternative hypotheses. Let’s try to visualize this. We are saying that the null hypothesis is there is no difference (or less aggression), but at some point no difference turns into greater difference. Furthermore, we have a directional hypothesis in that we do not think the difference will be negative, that children watching the adult play aggressively will exhibit fewer aggressive behaviors. Basically, we need to know what the critical value is in the figure below, which is the point at which we say “Okay, this difference is big enough that I will reject the null hypothesis.” Figure 1: Critical area of statistical significance This visual is for a one-tailed, directional hypothesis with an alpha of 5%. Therefore, the critical region is only on one side and covers 5% of the area under the curve. If our test had been for a two-tailed, non-directional hypothesis with an alpha of 5% then the critical region would be on both sides of the curve, with 2.5% of the area under the curve on both sides. If our test had been one-tailed with an alpha of 1%, then the critical region would be further to the right (larger, and in fact would be 2.048, which you can find in the table below) and would only be 1% of the area under the curve. We figure out that critical value based on what we set as our level of significance, also known as the alpha level. Most studies you read use the arbitrary \\(\\alpha\\) = .05 (or 5%), although we really should be thinking critically about what alpha level we use (more on that in the next chapter). In the visualization above, we set the alpha to 5% and so the area shaded in red is exactly 5% of the area under the curve of the normal distribution. Our alpha is the level of which we are saying would be considered “surprising” versus “not surprising.” If we got a mean difference that fell in that red area, then we would consider that “surprising” if we believed the null hypothesis was true. Basically, if we assume there is a mean difference of 0 (i.e., the null hypothesis), values past the critical value would be considered surprising enough that we would say that we reject the null hypothesis. In other words, the area in red are values that are unlikely to occur if the null hypothesis (in this case, mean difference &lt;= 0) were true. Set the criteria for obtaining the critical t-value Now that we understand that a bit better, how do we find out our critical value, or the values in the red region? Back in the day before computers, some fancy mathematicians and statisticians figured out the exact t-values based on things like the direction of our hypothesis, our alpha, and our degrees of freedom. Let’s figure these out for our example: Direction of our hypothesis: We have already determined we’re using a one-tailed hypothesis because we have a directional hypothesis. We think children in the aggression condition will have more aggressive behaviors than children in the passive condition. Alpha: Let’s stick with the default that most psychological research uses of \\(\\alpha\\) = .05 Degrees of freedom: For this test, this is calculated by n1 + n2 - 2, or N - 2. We have 30 children total, so 30 - 2 is 28 In our course, we will always assume that alpha is 5%. However, our next chapter will explain more about when we might change our alpha, typically to something lower like 1% or .5% or .1%. Determine the critical t-value We then go to a t-table like the one below and find the cell we are looking for to identify our critical t-value (tcrit) which we will then compare to our obtained t-value (tobt) in the 4th step. First, we have a one-tailed hypothesis and our alpha probability is set at .05 so we’re going to look under the sixth column (t.95, one-tail = .05). Then we need to find the row for our degrees of freedom (df = 28). That leads us to our critical t-value, a tcrit of 1.701. If our alpha was still .05 but we had a two-tailed test, then we would look under the seventh column (t.975, two-tails = .05). If our alpha was .01 and one-tailed, we’d look at the eighth column, and if it was .01 and two-tailed we’d look at the ninth column. Calculating the exact t-score and p-value We’re going to figure out how to get our obtained t-value (tobt) results based on the data from our study. We had 30 participants, 15 in each condition. Imagine that the researcher performed the experiment and got the following results: Children who watched the video of the adult playing aggressively with the Bobo doll displayed an average of 51.10 aggressive behaviors (SD = 3.50). Children who watched the video of the adult playing passively with the Bobo doll displayed an average of 27.40 aggressive behaviors (SD = 3.30). The mean difference is 51.10 - 27.40 = 23.70. We’ll learn how to conduct a t-test in jamovi later, but for now you can just input the numbers into this calculator (note: select to Enter mean, SD and N and choose the test Unpaired t test). It nicely gives you a lot of the values, but the one we are looking for is the test statistic, which is tobt = 19.08 (notice we round to two decimals in APA style). The calculator also gives us our exact p-value, which in this case is &lt; .00001 but in APA style we typically report p-values to three decimal places, so we would say p &lt; .001. The probability of getting a t-value as large as we did is less than .1% (less than our alpha of .05, so it is statistically significant). Very surprising! This calculator is acting as a statistical software and therefore giving us our exact p-value just like jamovi will provide for you once we move to the software. For now, just appreciate that we can estimate whether we met the p-value threshold when we analyze things by hand below. Step 4. Interpret the results We then need to compare our critical t-value (tcrit = 1.701) to our obtained t-value (tobt = 19.08): When tobt &gt; tcrit then the results are statistically significant (p &lt; α) and we reject the null hypothesis. When tobt &lt; tcrit then the results are not statistically significant (p &gt; α) and we fail to reject the null hypothesis. Since 19.08 &gt; 1.701, we reject the null hypothesis that there is no difference in conditions or that children in the passive condition displayed more aggression than children in the aggressive condition. A common mistake is assuming that p &lt; .05 means that the alternative hypothesis is true. This is inaccurate because the p-value is the probability of our data given the null hypothesis is true. It says nothing about the alternative hypothesis. Similarly, a common mistake is assuming p &gt; .05 means the alternative hypothesis is false. This is incorrect for the same exact reason; NHST says nothing about the alternative hypothesis. Type 1 and 2 errors However, this is when Type 1 (false positive) and Type 2 (false negative) errors come into play. Just because we get a result does not automatically mean that result is 100% accurate. There are many things that could lead us to an inaccurate interpretation! Perhaps it was a fluke, perhaps you didn’t do something correct in your study, or perhaps the effect does or does not actually exist. I like to use this table when discussing errors. On the far left column, we have our results: were they statistically significant (p &lt; .05) or not (p &gt; .05)? On the top row, we have whether in the real world the null or alternative hypothesis is true. In reality, we can never truly know whether the null or alternative hypothesis is true. We can at best approximate our understanding of the real world through replication! H0 is true H1 is true p &lt; .05 (statistically significant) Type 1 error Correct interpretation p &gt; .05 (statistically non-significant) Correct interpretation Type 2 error Therefore, any time we get a statistically significant result (p &lt; .05), then either we made a correct interpretation or we made a Type 1 (false positive) error! Similarly, any time we get a statistically non-significant results (p &gt; .05) then either we made a correct interpretation or we made a Type 2 (false negative) error! Another common mistake is to assume statistically significant results mean the null hypothesis is false. However, we can never know beyond a certainty of a doubt whether a significant result is a correct interpretation; we could also be making a Type 1 error. Similarly, a common mistake is to assume non-significant results mean the null hypothesis is true. We never know for sure whether the null or alternative is true, so in this case we could be making a Type 2 error. If you’d like to read a more in-depth discussion of error control, I recommend also reading Daniel Lakens’ chapter in his textbook “Improving Your Statistical Inferences.” Next week we’ll learn a lot more about p-values, errors, and more. For now, tuck this piece of information into your brain to remember! Write up the results in APA format This is described in more detail in the Writing up results chapter, but when you are done performing a test and interpreting the results then you need to write up the results for your paper or assignment. The study design was actually much more impressive than what I’m describing. They accounted for the children’s baseline aggression and the gender of both the child and the person in the video. If you are interested, you can read more here: https://www.simplypsychology.org/bobo-doll.html↩︎ "],["final-note-about-hypothesis-testing.html", "7.2 Final note about hypothesis testing", " 7.2 Final note about hypothesis testing When you read journal articles, you’ll note that they rarely discuss the null or alternative hypothesis. They may explain their research questions or their hypotheses (these hypotheses are their alternative hypotheses), but they rarely discuss the null. This is not necessarily a bad thing. Rather, what may be problematic with it is if researchers apply NHST without critically thinking about what their null hypothesis is or whether they have a one-sided hypothesis, which leads researchers to use defaults when the defaults may not be most appropriate. However, it would probably be a better thing if everyone clearly specified their alternative and null hypotheses if they are doing NHST. Also, you may have heard some things about p-values not being reliable or desirable. We’ll discuss what p-values are and how some researchers want to move away from them in the next chapter. In the meantime, just know that p-values are often misunderstood and therefore misused, and a lot of the recommendations for moving away from p-values are just replacing one potentially problematic approach with another. "],["bean.html", "8. BEAN", " 8. BEAN What a random chapter title, right? Yes, but it’s also an important acronym in hypothesis testing. BEAN stands for: Beta (AKA power, which is technically 1 - \\(\\beta\\)) Effect size Alpha N (AKA sample size) The following sections will go through each of these in turn before ending with discussion on how all of these things interrelate. Although the BEAN acronym is useful (you’ll find out why in a later part of this chapter), I won’t discuss them in that particular order. "],["effect-sizes.html", "8.1 Effect sizes", " 8.1 Effect sizes An effect size is a quantitative description of the strength of a phenomenon (phenomenon means thing being studied). The larger the value, the stronger the phenomenon (e.g., bigger mean differences or stronger relationship). Whereas the p-value tells us about the statistical significance of an effect, the effect size tells us the practical significance, or how much we should care about the significance. Types of effect sizes There are two basic effect sizes we tend to talk about: The d family of effect sizes are standardized mean differences. They start at 0 (no mean difference) and can go up to infinity, with larger values meaning larger standardized mean differences. Some of the effect sizes in this family: Cohen’s d is perhaps the most popular standardized mean difference effect size. Generally, the equation is the mean difference divided by the pooled standard deviation, but in reality the equation differs based on a variety of scenarios and whether you are using a one-sample, independent samples, or paired samples t-test. Hedge’s g is a less biased version of Cohen’s d. Cohen’s d is particularly problematic for small sample sizes, so Hedge’s g is generally preferred, but you’ll see that not all statistical programs provide this effect size. It’s not that difficult to calculate Hedge’s g based on Cohen’s d, but just keep this information in mind. The r family of effect sizes are measures of strength of association. They range from -1 to +1 such that 0 is no relationship and |1| is a perfect relationship between the variables. As you’ll read about in the correlation and regression chapters, this family of effect sizes can describe the proportion of variance explained by squaring the correlation (e.g., with a correlation of r = .8, then the r-squared is .82 or .8 * .8 which is 64% variance explained). Some of the effect sizes in this family: r is a correlation. It’s a standardized measure of the strength of association where r = -1 or +1 means a perfect relationship and r = 0 is no relationship at all. We typically work with Pearson’s correlation, but we will also learn about Spearman correlation and rho. \\(\\eta^2\\) (eta-squared) measures the proportion of variance in the dependent variable associated with the different groups of the independent variable. This is considered a biased estimate, especially when trying to compare values across studies, so there are two more preferred effect sizes. We’ll cover the difference between these three in a later chapter (ANOVA). \\(\\eta^2_p\\) (partial eta-squared) is calculated slightly differently and is considered a less biased estimate (again, we’ll learn about this in a later chapter). This can allow for better comparisons of effect sizes across studies. It’s still not perfect, though. \\(\\omega^2\\) (omega-squared) is calculated even more differently and is considered the least biased estimate. There is also \\(\\omega^2_p\\) (partial omega-squared) and \\(\\omega^2_G\\) (generalized omega-squared) but as jamovi doesn’t provide it we won’t go over it in this course. What is all this about more or less biased effect sizes? It has to do with how the standard deviations are calculated in these effect sizes and the fact that we’re dealing with samples and trying to infer the population’s effect size. Cohen’s d and eta-squared tend to slightly overestimate the true population effect, so there are options that provide corrections for this overestimation and lead to less biased results. We’ll also learn about phi and Cramer’s V as effect sizes for the chi-square test and beta as an effect size for regression in subsequent chapters. If you nerded out over this information and want to learn more, check out this great journal article by Daniel Lakens or this chapter on effect sizes by Daniel Lakens. Small, medium, and large effect sizes What is considered a small, medium, and large effect size? Quite frankly, it depends. You may have seen some heuristics online about what small, medium, and large is for Cohen’s d (e.g., .2, .5, and .8) and r (e.g., .1, .3, and .5) but these heuristics should not be used without critical thought. In fact, Cohen (who is regularly cited for these heuristics) said that the way we should determine cut-offs is based on looking across studies to find what is considered small, medium, and large in that particular context. As an example, a correlation of r = .03 might be considered so small to be not meaningful, but it’s in fact the correlation between taking aspirin after a heart attack and the prevalence of a future heart attack7 and was considered so practically important that it’s now standard practice. Think why: taking aspirin after a heart attack is easy and cheap, and if it reduces to chances of a future heart attack even slightly then it has more benefit than harm. Let’s take another example. Let’s imagine that we have created an educational intervention that has a d = 1.00 increase in students’ GPAs. That’s big, right? Yup, but imagine that educational intervention costs $100,000 per student. That would be way too expensive to be practical. It would be unlikely that anyone would use the educational intervention, instead looking to see how they could reduce the cost of the intervention while keeping the biggest possible effect on students’ GPAs. What makes an effect practically significant? We’ll get into p-values in a moment, which are about statistical significance, but they don’t tell us anything about how meaningful the effect is. That’s what an effect size is for. But how do we know if it’s meaningful or practically significant? Lakens (who also did the great journal article on effect sizes above) has a fantastic new preprint out on Sample Size Justification. In it, he provides an overview of six possible ways to determine which effect sizes are interesting: “Smallest effect size of interest: what is the smallest effect size that is theoretically and practically interesting? Minimally statistically detectable effect: given the test and sample size, what is the critical effect size that can be statistically significant? Expected effect size: which effect size is expected based on theoretical predictions or previous research? Width of confidence interval: which effect sizes are excluded based on the expected width of the confidence interval around the effect size? Sensitivity power analysis: across a range of possible effect sizes, which effects does a design have sufficient power to detect when performing a hypothesis test? Distribution of effect sizes in a research area: what is the empirical range of effect sizes in a specific research area, and which effects are a priori unlikely to be observed?” (p. 3) Basically, what does past research have to say about what effect size you can expect (#3 and #6)? What is the smallest effect size you care about (#1)? What is the smallest effect size you can reasonably obtain (e.g., due to sample size limitations; #2, #3, and #4)? This is the justification you use to determine what effect size you are looking for. This is important for when you are then determining what sample size you need, which will be discussed in a separate section. As a fun followup, as an example of #6, this study in the field of education collected effect sizes of many education interventions to figure out benchmarks for small (&lt;.05), medium (&lt;.20), and large (&gt;= .20) effect sizes based on existing data rather than poor quality heuristics. More recent studies have actually shown that the relationship between aspirin and future heart attacks is actually a lot stronger than r = .03, but the point here is to show that even what appears to be a very small correlation isn’t very meaningful.↩︎ "],["alpha-p-values.html", "8.2 Alpha &amp; p-values", " 8.2 Alpha &amp; p-values p-values Whereas effect sizes tell us about practical significance, they do not tell us about statistical significance. That is what p-values are for: they tell us whether our results are statistically significant or how surprising they are. The formal definition of a p-value is that it is the probability of observing data that is as extreme or more extreme than the data you have observed, assuming the null hypothesis is true. There’s two things to keep in mind about this definition: The p-value is about the probability of our data. It is not about the probability of our hypothesis. The p-value is based on the assumption that the null hypothesis is true. In null hypothesis significance testing, we are only ever testing against the null. We can never “accept” the alternative hypothesis but rather reject the null. If we reject the null enough times (and rarely fail to reject the null) then it gives weighted evidence towards our alternative hypothesis, but we can never prove the alternative hypothesis is true. In APA style, we report the exact p-values rounded to three decimal places. In your jamovi settings, you can set the statistical significant reporting to “3dp” to satisfy APA requirements. Another way we could think of the p-value is: assuming there is no difference (i.e., the null hypothesis is true), how surprising is our data? Courtesy of Dr. Jess Hartnett (@Notawful on Twitter) You may have heard of p-values before, including some bad things (read the end of this chapter for more info). You may then be wondering why we are learning about p-values if they seem so problematic that they should be banned. The biggest problem with p-values is that they are misunderstood, even by researchers. They are often misinterpreted. Daniel Lakens has a great blog post on the topic and a great online course about improving your statistical inferences. He also has a great article that “The Practical Alternative to the p Value Is the Correctly Used p Value.” What are common misconceptions about p-values? Lakens’ course has a great assignment called “Understanding common misconceptions about p-values” that comes from Daniel Lakens. This section will summarize those misconceptions. Misunderstanding 1: A non-significant p-value means that the null hypothesis is true. A common version of this misconception is reading a sentence such as ‘because p &gt; 0.05 we can conclude that there is no effect’. Another version of such a sentence is ‘there was no difference, (p &gt; 0.05)’. Misunderstanding 2: A significant p-value means that the null hypothesis is false. (Lakens, 2017) These are both misconceptions because the p-values only tell us about the probability of our data, not the null or alternative hypothesis. We never know whether the null or alternative hypothesis is true, and we could always be making a Type 1 or Type 2 error depending on whether our results are statistically significant or not. Misunderstanding 3: A significant p-value means that a practically important effect has been discovered. (Lakens, 2017) p-values tell us about statistical significance; only effect sizes can tell us anything about whether the effect is practically important. Misunderstanding 4:If you have observed a significant finding, the probability that you have made a Type 1 error (a false positive) is 5%. (Lakens, 2017) Our p-values tell us nothing about the probability of a Type 1 error or Type 2 error. If we set our Type 1 error rate (alpha) to 5%, and if the null hypothesis is true (which, again, we technically will never know for real), then every 1 in 20 (5%) studies examining this effect will be statistically significant and will be a Type 1 error. Note that it’s about the rate of errors across studies and it’s something we set ahead of time. The p-values, again, are about the probability of our data! Are p-values bad? Some have argued that we should abandon the p-value; this has led to things like journals completely banning p-values altogether like Basic and Applied Social Psychology. However, I agree with Lakens that “the practical alternative to the p-value is the correctly used p-value.” That’s to say: there is nothing wrong with the p-value inherently, and it can be useful. Rather, what’s wrong is that many people use them incorrectly. The journal Basic and Applied Social Psychology banned the use of p-values, claiming that NHST is invalid. Instead, they want researchers to focus on effect sizes and appropriate power analyses. Although these are laudable attempts–I agree that researchers should have highly powered studies and focus more on practical significance over statistical significance–I agree more with researchers like Daniel Lakens who point out that the problem is researchers want NHST to do what it does not do. They want statistical procedures that can confirm a hypothesis, which NHST cannot do. While larger samples and a focus on effect sizes can help, we should trust NHST to do what it can do, which is to tell us about the probability of the surprisingness of our data if the null hypothesis is true. Read more about the journal’s statement here: https://www.tandfonline.com/doi/full/10.1080/01973533.2015.1012991 Alpha The p-value is the probability of the “surprisingness” of your data. But what is considered surprising enough? That’s where alpha comes in, which determines our critical region of significance. The figure below should look familiar; the area in red is the critical value, otherwise known as alpha, which is set by the researcher to determine what is considered surprising. The alpha level is the level at which we consider the data so surprising that we reject the null hypothesis. Everything in the red region is our alpha and signifies the region of statistical significance. Therefore, whether a p-value is statistically significant depends on what our alpha is set at. Figure 2: Critical area of statistical significance Why is alpha usually set at 5%? It comes from Fisher (1925), who said something that eventually grew to tradition: A deviation exceeding the standard deviation occurs about once in three trials. Twice the standard deviation is exceeded only about once in 22 trials, thrice the standard deviation only once in 370 trials…. The value for which P = .05, or 1 in 20, is 1.96 or nearly 2 ; it is convenient to take this point as a limit in judging whether a deviation is to be considered significant or not. Deviations exceeding twice the standard deviation are thus formally regarded as significant. Basically, .05 was convenient. It was 1/20. It was around 2 standard deviations from the mean in a normal distribution. For some reason, it caught on (maybe the “formally regarded as significant” was why). However, a year later, even Fisher acknowledged we shouldn’t just arbitrarily use p = .05 as our alpha level.8 Rather, we should consider setting it at higher odds (e.g., p = .01). He also argued, “A scientific fact should be regarded as experimentally established only if a properly designed experiment rarely fails to give this level of significance.” (Fisher, 1926, p. 504). In other words, we need to think critically about the alpha level we set and we need to test an effect multiple times before we start thinking the alternative hypothesis is true. We’ll discuss power next before we start putting it all together. What value do I set alpha at? In this course, unless I specify otherwise, we will use an alpha of .05. However, in reality, how do you choose your alpha level? There’s a bit more to it than we’re going to cover in this course, but it has to do with both your alpha and power level (and we haven’t covered power yet), and the probability that the null or alternative hypothesis is true (which we can never truly know). This is not completely accurate, but generally speaking if you are more concerned with a false positive (type 1 error) then lower your alpha, and if you are concerned with a false negative (type 2 error) then increase your power. What does this look like in practice? In a Psi Chi article by Kobza and Salter (2016), they stated, “Due to the high number of statistical tests conducted, a lower p value (.01) was used to determine significance.” (p. 75). If you find more examples of researchers changing their alpha values, please send them to me so I can add them here! How do I know what alpha researchers are using? Ideally, all researchers should be reporting their alpha values in all their publications. For example, in a Psi Chi article by Campero-Oliart and colleagues (2020), they stated, “Alpha levels for all analyses were set to .05.” (p. 47). In reality, we typically don’t report anything and assume a default of .05 without explicitly stating so (and yes, I am also guilty of this). However, you should always be thinking critically of your alpha value and, if you are using something other than .05, you should explain what level you choose and why. Videos and resources The following video walks through some of the effect sizes, alpha, and power stuff. You might also like this chapter or video on p-values by Daniel Lakens. Another video recommended by a student is this one from zedstatistics on p-values. If you want to read more, this is a short read on the history of the .05 alpha level: https://www2.psych.ubc.ca/~schaller/528Readings/CowlesDavis1982.pdf↩︎ "],["power.html", "8.3 Power", " 8.3 Power Power is the probability that you will observe a significance effect if there is a true effect. In other words, power is the probability of a statistically significant result assuming the alternative hypothesis is true. Let’s compare this to our definition of the p-value: the probability of observing data that is as extreme or more extreme than the data you have observed, assuming the null hypothesis is true. Power can range from 0-100%, but typically people set it at 80%. This means that, if we set power at 80% and if the alternative hypothesis is true, then we have an 80% chance of detecting the effect. However, in practice, power is often far lower than 80% due to insufficient sample sizes for the effect sizes of interest, something we’ll investigate in the final section of this chapter and in the homework. Just like with alpha, you have to critically set your power level too. Although we commonly set it to 80%, that still leaves a large likelihood of a false negative if the alternative hypothesis is true. To better detect a statistically significant result (assuming the alternative is true) then we should aim for higher power levels. Power is based on the assumption that the alternative hypothesis is true whereas the p-value is based on the assumption that the null hypothesis is true. If we want to increase the likelihood of supporting our alternative hypothesis, then we should be doing all we can to increase our power! B(E)A(N): Alpha and power Let’s start putting all this together. Here’s all our definitions so far (remember, we can never truly know whether the null or alternative hypothesis is true): Alpha is the value we set to determine what constitutes a statistically significant result, assuming the null hypothesis is true. Power is the value we set at for what constitutes a statistically significant result, assuming the alternative hypothesis is true. A type I error is when we get a statistically significant result but in fact the null hypothesis is true. A type II error is when we do not get a statistically significant result but in fact the alternative hypothesis is true. A correct inference is when we either get a statistically significant result when the alternative hypothesis is true or when we do not get a statistically significant result when the null hypothesis is true. In the following table, determine where each of the pieces should go. Note that we have six things to populate but only four cells: each cell must contain at least one of the six things. Think critically here before checking your answers! H0 is true H1 is true p &lt; .05 (statistically significant) A B p &gt; .05 (statistically non-significant) C D Which cell should each of the following items go? alpha power type I error Type II error Correct inference (hint: there are 2!) How alpha and power relate to one another We’ll eventually see how power, alpha, effect sizes, and sample size all interrelate (BEAN!). For now, let’s examine how alpha and power relate to one another in the context of hypothesis testing. Note that a lot of this comes from Daniel Lakens’ Improving your Statistical Inference work. Remember: we never know for certain if the null or alternative hypotheses are true! These are for illustrative purposes, and you can use the following if you have some idea about the probability of one over the other. For example, if many studies have found a particular effect you might put more weight on the alternative hypothesis is true (e.g., 75%). As another example, if you are doing a completely novel study you might put equal weight (50%/50%) on both hypotheses. Assuming the null hypothesis is 100% true Assuming the null hypothesis is 100% true, we could fill in the table with actual numbers. Let’s also use the arbitrary values we often set alpha and power at: alpha = 5% and power = 80%. Here’s the resulting table: H0 is true H1 is true p &lt; .05 (statistically significant) 5% 0% p &gt; .05 (statistically non-significant) 95% 0% How did I get there? First, we’re assuming the null hypothesis is 100% true. Therefore, that column must add up to 100%. If the null hypothesis is 100%—and we know our hypotheses must be mutually exclusive—then the alternative hypothesis must be 0% true. Therefore, that column must add up to 0%. The whole table must equal to 100% to exhaust all options. Therefore, our power doesn’t matter at all in this case. If the null is true, then it doesn’t matter what power we have to detect the alternative effect because the alternative effect does not exist. So we instead use alpha and put it in the upper left cell. Note that our alpha level is the Type I error rate we are setting! If the whole table must equal to 100%, and the left column must equal to 100% because the null is 100% true, then 100-5 = 95% for the correct inference. In other words, if we tested this effect (that doesn’t exist) 100 times, around 95% of the time we would get a non-significant p-value (p &gt; .05) and about 5% of the time we would get a significant p-value and be making a Type I error. We can visualize our p-value distribution using this handy interactive calculator. We set our effect size (ES) to be d = 0, meaning there is no effect (i.e., the null hypothesis is true). This results in a uniform distribution of p-values. Exactly 5% of p-values would fall between p = 0 and p = .05 (the shaded region to the right of the red dotted line). That aligns with our Type I error rate as well (5%). Go ahead and play around with the interactive calculator and try moving the slider for sample size! Notice that it does absolutely nothing. We’ll understand why when we put everything together. Assuming the alternative hypothesis is 100% true Let’s try out the opposite: assume the alternative hypothesis is 100% true, alpha is 5%, and power is 80%. What would you put in the table? H0 is true H1 is true p &lt; .05 (statistically significant) 0% 80% p &gt; .05 (statistically non-significant) 0% 20% How did I get those numbers? First, remember that the table must equal to 100% (hypotheses must be exhaustive). Second, remember that the alternative hypothesis is 100% true so that column must equal to 100% (and because hypotheses must be mutually exclusive, the other column must equal to 0%). Therefore, it doesn’t matter what we set alpha to. We cannot get a Type I error if the alternative hypothesis is true! We can only get a correct inference or make a Type II error. We set power to 80%, and power is the probability of getting a statistically significant result assuming the alternative hypothesis is true. Therefore it goes in the top right cell. Notice that power is the probability of correctly detecting a statistically significant effect! With simple arithmetic, 100-80 = 20% is our Type II error (aka a false negative). If we were to test for this effect 100 times, about 80 times we would correctly detect the effect and about 20 times we would fail to detect the effect. Let’s visualize this. Go back to our handy interactive calculator and put d = .8 as our effect size and n = 25 per group to our sample size. Notice now what our distribution of p-values looks like! Rather than a uniform distribution, now we have a steep exponential distribution. I have chosen to highlight all p-values in the range of p &gt; .05, which in that selection is roughly 20% (our Type II error rate from above). The number of p-values &lt; .05 is roughly 80%. Play again with the ES slider and sample size slider. Notice now that it makes a difference to our power! You’re getting a glimpse into how power depends on our alpha, effect size, and sample size. BEAN! Assuming a 50/50 split on the null and alternative hypotheses In reality, we never truly know whether the null or alternative hypotheses are true. Maybe we’re testing a new effect and we are completely 50/50 of whether the null or alternative hypothesis is true. Let’s keep our alpha and power the same (5% and 80%, respectively) and fill out our table now: H0 is true H1 is true p &lt; .05 (statistically significant) 2.5% 40% p &gt; .05 (statistically non-significant) 47.5% 10% How did we get there? Again: the table must equal to 100% and we specified ahead of time that we thought it was about 50% true for each of the hypotheses, so each column must equal to 50%. 50% of 5% (alpha) is 2.5% and 50% of 80% (power) is 40%. We then fill out the bottom row based on arithmetic. Imagine this were your study and you got a significant p-value. What could you conclude? Either you reject the null hypothesis or fail to reject the null hypothesis. But which one? In reality, we never know, but there are things we can do to increase the likelihood that our statistically significant result is because the alternative hypothesis is true and not the null hypothesis. Right now, based on the values we have set (alpha = 5% and power = 80%), it is 16 times more likely that a statistically significant result means the alternative hypothesis is true than the null hypothesis is true. We calculate this with the first row of data: the probability of a statistically significant result if the alternative is true divided by the probability of a statistically significant result if the null is true: 40%/2.5% = 16. You might be fine with a statistically significant result being 16 times more likely that the alternative is true compared to the null, but what can we do to increase this likelihood? Increasing power Remember: the researcher sets the alpha and power levels! Let’s find out what happens when we adjust those values. Let’s try it again, but this time let’s increase our power to 95% and keep our alpha at 5% (50/50 on the hypotheses). Fill out the table! H0 is true H1 is true p &lt; .05 (statistically significant) 2.5% 47.5% p &gt; .05 (statistically non-significant) 47.5% 2.5% Now it is 19 times more likely (47.5/2.5 = 19) that the alternative hypothesis is true than the null hypothesis is true. Awesome! We have now discovered that increasing power increases the likelihood that a statistically significant result means our our alternative hypothesis is true. Decreasing alpha Let’s do another example in which we still have a 50/50 on the hypotheses but we reduce our alpha to 1% and keep our power at 80%. Fill out the table! H0 is true H1 is true p &lt; .05 (statistically significant) .5% 40% p &gt; .05 (statistically non-significant) 49.5% 10% Now it is 80 times more likely (40/.5 = 80) that the alternative hypothesis is true than the null hypothesis is true. Awesome! We have now discovered that decreasing alpha, not increasing power, is the best way for increasing the likelihood that a statistically significant result means our our alternative hypothesis is true. Your turn: Increasing power AND decreasing alpha Let’s see what happens when we both increase power AND decrease alpha. Fill out the table on your own. When we assume the null and alternative hypotheses are 50% likely each, and we set our alpha to 1% and our power to 95%, how much more likely is it that the alternative hypothesis is true than the null hypothesis is true? H0 is true H1 is true p &lt; .05 (statistically significant) A B p &gt; .05 (statistically non-significant) C D Video The following walks through this power stuff some more. "],["sample-size-power-analysis.html", "8.4 Sample size &amp; power analysis", " 8.4 Sample size &amp; power analysis Sample size (N) Sample size is the total number of participants in a study (often denoted as n in a paper). In a between-subjects study, we should also describe how many participants are in each group, especially if the groups are unequal in size. Often, the biggest question we want to know is: what sample size do I need for my study? Daniel Lakens has a great new preprint and chapter out on the topic and a sample size justification online tool. We often cannot measure the entire population, but some other ways we can determine the sample size are: Resource constraints: sometimes time and budget limits our sample size Accuracy or an a priori power analysis: based on the statistical power we hope to achieve (which is in turn based on the effect size we expect) Heuristics: some prespecified rule or norm that is described in the literature (to be avoided as much as possible because they’re often wrong) The first option is more of a research methods discussion and will not be discussed here. The third option is to be avoided as much as possible. Therefore, that leads us to the second option, which is to conduct an a priori power analysis. We’ll cover it broadly here, but if you want to learn more about conducting power analyses in jamovi Bartlett &amp; Charles (2022) have a great preprint on the topic. BEAN: Power analysis We previously saw how alpha and power relate to one another. In the interactive calculator you started to discover that effect sizes and sample size also relate to alpha and power. This is the power of the BEAN: if you know three out of the four of BEAN, you can determine the fourth. Power, effect sizes, alpha, and sample sizes all interrelate! Typically, there are three things we may be interested in figuring out related to BEAN: What sample size do I need given the effect size of interest, alpha level, and power level? What power do I have to detect the effect size of interest given my alpha level and sample size? What effect size can I reasonably detect given my alpha level, power level, and sample size? There is software out there to help you conduct power analyses. The most popular is G*Power (there are various guides on how to use G*Power, including this and this). There are specific power calculators for more complex statistical procedures. There are also methods of using simulations to determine the power, but they are more complex and we won’t cover them in this course. For our purposes, we’re going to simplify things and use the jpower module in jamovi. This can calculate power for an independent samples t-test, a paired samples t-test, and a one-sample t-test. Our previous example in the last chapter (the Bobo doll experiment) has two groups in a between-subjects design. Next chapter you’ll learn how to determine what statistical test you would perform, but for now I will just tell you that we would conduct an independent samples t-test with this experiment’s data. In the jpower module, choose your statistical test in the drop-down menu; in this case, let’s choose independent samples t-test. Next, you specify what you want to calculate: your N per group (sample size), power, or effect size. It will grey out that box in the three boxes below. Let’s discuss them in turn: Drop-down menu: First choose whether you are solving for sample size, effect size, or power. Minimally-interesting effect size: it shows the lower case Greek letter delta here, but we can essentially think of it as a Cohen’s d value. Go back to the effect size section for help in determining your smallest effect size of interest. Minimum desired power: remember from the last section that when we increase power, we increase the likelihood of both obtaining statistically significant results and the likelihood that a statistically significant result because the alternative hypothesis is true than that the null hypothesis is true. N for group 1: this is the sample size in one of your two groups. Relative size of group 2 to group 1: if your sample sizes are equal in both groups, leave it at 1. If they aren’t, you need to figure out the ratio. For example, if one group is n = 20 and the other is n = 40 then you would change this box to “2”. You can easily calculate this by dividing n2 by n1. \\(\\alpha\\) (type I error rate): remember from the last section that when we decrease alpha, we increase the likelihood of obtaining non-significant results when the null hypothesis is true and increase the likelihood that a statistically significant result means the alternative hypothesis is true. You probably shouldn’t increase it above .05, but you should consider whether it would be useful to decrease it in your case. Tails: specify whether you have a two-tailed (non-directional) or one-tailed (directional) hypothesis. There are also options for four types of plots and whether to have explanatory text. For now, keep the explanatory text checked because it will help explain what is going on in the results. The plots are optional and I encourage you to check them out to see if they help you understand what is going on. Power analysis example #1 Let’s return to our example that we used in our interactive calculator before. We are going to calculate Power, set our effect size at \\(\\delta\\) = .8, N for group 1 at 25, Relative size of group 2 to group 1 at 1 (equal sample sizes), and \\(\\alpha\\) (type I error rate) to .05. We’ll assume we have a two-tailed hypothesis for now. You should get the following results. This table specifies that we defined the sample size, effect size, and alpha, which results in a power calculation of 79%. The results also provide a useful explanation: A design with a sample size of 25 in each group can detect effect sizes of δ≥0.8 with a probability of at least 0.791, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of a=0.05. This assumes that an effect size of .8 is the smallest effect size of interest. The next table shows us the power to detect various other effect sizes based on our alpha and sample size: True effect size Power to detect Description 0 &lt; d = 0.566 ≤50% Likely miss 0.566 &lt; d = 0.809 50% – 80% Good chance of missing 0.809 &lt; d = 1.041 80% – 95% Probably detect d = 1.041 ≥95% Almost surely detect In other words, we are almost sure to detect really large effect sizes (d &gt; 1.04), but we’ll likely miss really small effect sizes (d &lt; .56). This gives us a good hint to the relationship among BEAN: holding alpha and sample size constant, as effect sizes go up power goes up. The Power Contour plot can show a bit more about how power (color), effect size (y-axis) and sample size (x-axis) all relate to one another. Notice how the x-axis is not linear. We are learning some more about the relationship among BEAN: increasing our sample size increases our power, holding alpha and effect size constant. The next two plots are basically the Power Contour plot, but they shift power to the y-axis and either show effect size or N on the x-axis. The last plot (Power Demonstration) helps us visualize our Type I and Type II errors and correct inferences nicely. The purple distribution is our null hypothesis distribution (centered at d = 0) and the green distribution is our alternative hypothesis distribution (centered at d = .8). The vertical dashed lines are the critical values of obtaining p &lt; .05 on either side of the null distribution (so 2.5% on either side). The dark green area is therefore our power (80%) and the dark purple areas are our Type I error rate (5%). The light green area to the left of the dashed line is our Type II error rate (20%) and the light purple area is the probability of a correct inference assuming the null is true (95%). Keep in mind that these are the distributions of both hypotheses though, and in reality only one is true. We can just never know which is true; we can at best approximate it through repeated testing of effects. Play with jpower Play around some more with jpower. Try calculating other things (e.g., sample size or effect size). Play with power and see what increasing it does to your effect size and sample size. Play with effect sizes and see what decreasing them does to your power and sample size. Play with alpha (don’t go higher than .05) and see what that does to your power, effect size, and sample size. And lastly, play with your sample size and see what it does to your power and effect size. Your assignments for this unit will have you conduct power analyses based on various scenarios, so playing around with jpower will help prepare you for them. Extending our knowledge of power analysis If you’d like to learn more about power analyses in a variety of experimental designs using G*Power or R packages, I recommend this article by Perugini et al. (2018) called “A practical primer to power analysis for simple experimental designs.” "],["inferential-statistics.html", "9. Inferential statistics", " 9. Inferential statistics We’ve learned about hypothesis testing for inferential statistics two chapters ago and learned about some specific components of statistical testing in the last chapter. We have alluded to the fact that there are multiple inferential statistics we can perform, and that is the purpose of this chapter. Although there are many more types, we are going to cover two basic types of inferential statistics: Parametric statistics, which have an assumption of normal distribution Non-parametric statistics, which have no assumptions about the distribution of the data "],["choosing-the-correct-test.html", "9.1 Choosing the correct test", " 9.1 Choosing the correct test It is important that you learn how to identify which inferential statistic you should perform. This chart can help you determine what statistical test to perform. Note that on the right dark red boxes are parametric tests, light red boxes are non-parametric tests, and white boxes will not be covered in this class at all (in fact, there are many others not even shown that we won’t cover!). Data types are indicated in either blue (continuous), green (categorical), or teal (both). Number of variables or levels of the variables are either 1 (light orange), 2/2+ (orange), or 3+ (dark orange). Between-subjects designs, meaning designs with different participants in each group, are in black whereas within-subjects designs, meaning designs with the same participants in each group, are in light grey. Note: if you are a student in PSYC 290 or 790, you have access to a high-resolution PDF of this chart on Canvas. You may find it helpful to first think whether you have a between-subjects or a within-subjects design. If you have groups, are participants in the same conditions (within-subjects) or different conditions (between-subjects)? Then that narrows down the possibility of what inferential statistic you would use. For example, if you know you have a repeated measures design with only one IV, then it’s either a dependent t-test or repeated measures ANOVA depending on whether there are 2 or 3+ categories to the IV. First, you need to determine what level of measurement your dependent variable (DV) is. We will only be covering statistical tests that have one dependent variable. Therefore, you need to know whether the variable is categorical (i.e., nominal or ordinal) or it’s continuous (i.e., interval or ratio). Go back to Chapter 2 “Levels of Measurement” to refresh yourself on the difference between categorical and continous variables! This is very important information to remember throughout this course. Furthermore, you need to know exactly how things are measured or manipulated to determine the level of measurement. Next, you need to determine how many independent variables (IVs) there are and then what level of measurement your IV(s) are. In the case of a single categorical IV, we also need to know how many levels there are to the IV (i.e., how many categories there are). For categorical variables, we also need to know if the participants are different (i.e., between-subjects) or the same (i.e., within-subjects) within each level of the category. Lastly, for many of the statistical tests we need to know whether we meet the assumptions of parametric tests. If we don’t meet the assumption, then there are alternative tests we can perform. We’ll learn about assumption checking in the next section of this chapter. Checking assumptions is very important to know which exact test to perform. For now, we’ll assume the assumptions are met, but we’ll learn more about checking assumptions in the next chapter. Another tool that can help you determine the statistical test you want to perform is https://stathand.net/ We can both forward map and backwards map with the chart above. Forward mapping involves understanding your data and your research question and then determining what statistical test to perform. Forward mapping is mostly what you need to understand how to do! Backwards mapping involves determining what kind of data is needed to perform a particular statistical test. This is more for educational and understanding purposes and generally is not how you analyze data. You may also use it to justify which statistical test you performed. Let’s do some examples of forwards mapping. You may want to read the example and try your hand at it first and then check your answers! Forward mapping: Choose the correct test A researcher is interested in understanding whether athletes have higher English scores than non-athletes. In other words, what is the effect of athletic status on English test scores? What is the DV? What is the level of measurement? It’s English test scores, which is a continuous variable. How many IVs are there? We only have one IV, and it is athletic status. What is the level of measurement of the IV? Athletic status is a categorical variable. How many categories to the IV? Athletic status is measured as either athlete or non-athlete, so there are 2 levels. Are the same or different participants used in each category? People can either be an athlete or not an athlete, but they can’t be both, so this is a between-subjects variable (aka “different”). Do data meet the assumptions for parametric tests? We don’t know. We would need to test this. For now, let’s assume we meet the assumptions. Statistical test? Independent t-test A researcher is interested to know whether people perform better on the exam at the start, middle, or end of the semester. The researchers has all participants complete all three exams. What is the DV? What is the level of measurement? In this case, the exam is our DV and it’s a continuous variable. How many IVs are there? We only have one IV, and it is time of the exam. What is the level of measurement of the IV? The time of the exam is a categorical variable. How many categories to the IV? Type of test has three categories: start, middle, or end of the semester. Are the same or different participants used in each category? Although the researcher could have designed a between-subjects design, this particular study has all participants participate in all conditions, so it is a within-subjects design (aka “same”). Do data meet the assumptions for parametric tests? We don’t know. We would need to test this. For now, let’s assume we meet the assumptions. Statistical test? One-way repeated measures ANOVA Backwards map: Determine the data you need Let me start off by saying we don’t normally do this. We perform the test based on the data we have. But in our learning, we also want to ensure we learn all the tests. Imagine I gave you a dataset and wanted you to perform two different tests that I told you about. Here are the variables in the dataset: Mile time (continuous variable ranging from 5-30 minutes) BMI (categorical variable of underweight, normal, or overweight) Happiness at the start of the semester (continuous variable ranging from 0-10) Happiness at the end of the semester (continuous variable ranging from 0-10) If I told you I wanted you to perform a dependent t-test, what data would you use? Assuming we meet the assumptions for a parametric test, we need to find a situation in which we have 1 continuous variable and 1 categorical variable with 2 levels in which participants are the same within each category (i.e., within-subjects variable). We only have three continuous variables: mile time and our two happiness variables. If we rethink happiness, we can realize that it’s really a within-subjects variable. We are measuring happiness (our continuous DV) across two time points (start and end of the semester). Therefore, we could perform a dependent t-test with our two happiness data points and see whether happiness differs across time in the semester. If I told you I wanted you to perform a one-way independent ANOVA, what data would you use? Assuming we meet the assumptions for a parametric test, we need to find a continuous DV and a single categorical IV with 3 or more levels in which participants are different across categories (i.e., between-subjects design). We only have three continuous variables: mile time and our two happiness variables. We only have one categorical variable (BMI), and it has 3 levels: underweight, normal, overweight. Now this is where we need to think critically. What would be the most interesting test here? How BMI affects happiness or how BMI affects mile time? Weight and performance on running a mile seem to make most sense here. Therefore, we could look at how BMI affects mile time. Though keep in mind we are not randomizing here and so this is not an experimental design! "],["parametric-assumptions.html", "9.2 Parametric assumptions", " 9.2 Parametric assumptions Most of the inferential statistics we’ll be learning in this class are parametric statistics which means it’s based on certain assumptions about the shape of the distribution (in this class we’ll focus on the normal distribution). It’s important to check assumptions because your data may not always be what they seem. You need to look at your data in all sorts of ways to make sure it’s satisfactory for inferential statistics. This video is a fun little look into how our eyes can deceive us and why we need to look at things from a variety of angles and perceptions. There are four basic assumptions of most parametric tests: Interval or ratio (i.e., continuous) dependent variable Independent scores on the dependent variable Normal distribution Homogeneity of variances Let’s discuss these in turn and how to test for them. Interval/ratio data If we are performing a parametric test, then the dependent variable (DV) must be measured at the interval or ratio level. It is important that the data has proportional intervals between levels of the variable, and ordinal variables often do not meet this assumption. It is very important to avoid treating ordinal variables as continuous variables. We cannot calculate a mean or difference between ordinal values, but we can for continuous variables. What is often done—and is often inappropriate to do—is treat Likert-scale items as a continuous DV. What we can do is take a sum or average of multiple Likert-scale items and treat that sum or average as a continuous DV (although some argue this is also inappropriate). There is no “test” we can perform here to test whether our data meets this assumption. Rather, you will need to just recognize whether data is interval/ratio (continuous) or ordinal/nominal (categorical). Independent scores In between-subjects designs (e.g., the independent t-test or one-way ANOVA), data from different participants should be independent meaning that the response of one participant does not influence the response of another participant. We violate this assumption in the case of nested data (e.g., when our sample consists of students in three different classrooms, it is likely that students within classrooms are more similar than we would expect otherwise). In within-subjects designs (e.g., the dependent t-test or repeated measures ANOVA), we automatically violate the assumption because of course the scores of one participant in one condition will relate to their scores on another condition. However, their scores should still not influence any other participant’s response. This is another assumption, like interval/ratio data, that we do not ever test but is a function of knowing our data, how it was collected, and whether one participant’s data is thought to affect another participant’s data. Normal distribution For all our statistics, our dependent variable needs to be normally distributed, or have a normal distribution. You may have also heard it called a bell-shaped curve. It has really important statistical properties which is why most of the inferential statistics we’ll be learning in this class are parametric statistics that assume our data has a normal distribution. Some of the important statistical properties of the normal distribution: Data are equally distributed on both sides of the mean. Skew and kurtosis are equal to 0, which is to say there is no skew or bad kurtosis. The mean is equal to the median, and both are the exact center of the distribution of data. In other words, if your mean and median are not the same, you know you have skewed data! In fact, if your median &lt; mean then you have positive skew and if your median &gt; mean then you have negative skew. We know the percentage of cases within 1, 2, 3, etc. standard deviations from the mean. There are four ways to test for normality: Visualize the distribution Test the skew and kurtosis Conduct a Shapiro-Wilk test Visualize the Q-Q plot We are going to learn about four different methods of checking normality. For your homework assignments, you should always use all four methods to check normality. Note that sometimes the four tests do not always agree with one another. For example, everything might look good but Shapiro-Wilk’s test is statistically significant or perhaps the visual distribution looks non-normal but everything else looks fine. In those cases, it is up to you as the researcher to make a judgment. Reasonable people do different things, but as long as you can back up your judgment then you’ll be fine. Personally, I tend to prioritize the visual inspection of the data over the other pieces of evidence, and I sometimes am more cautious than not just to be safe. Visualize the distribution In jamovi, we can go to the Explorations option and choose Descriptives. Under Plots, we can choose a histogram and/or density plot (figure on the left, which shows both) or boxplot and/or violin plot and/or data points (figure on the right, which shows all three). We look at this data and visually inspect with our eyes whether the data is normally distributed based on what we know a normal distribution (bell curve) looks like. Note that the violin plot is the density plot transposed to be vertical and mirrored! In our case, height looks pretty fairly normally distributed. Test the skew and kurtosis In jamovi, we can go to the Explorations option and choose Descriptives. Under statistics, choose skew and kurtosis. You’ll have to do a bit more work to actually figure out whether the skew and kurtosis is problematic though. For height, here is our skew and kurtosis: Descriptives Height Skewness .230 Std. error skewness .121 Kurtosis .113 Std. error kurtosis .241 We need to calculate z-scores for skew and kurtosis. We do that by dividing the value by its standard error: Skew: .230 / .121 = 1.90 Kurtosis: .113 / .241 = .47 How do we know if it’s problematic? If the z-score for skew or kurtosis are less than |1.96| then it is not statistically significant and is normally distributed. However, if the z &gt; |1.96| then it is statistically significant and is not normally distributed. In this case, both skew and kurtosis z-scores are less than 1.96 so we meet the assumption of normal distribution as evidenced by skew and kurtosis. Why 1.96? Refer back to the t-table we learned about in Chapter 4 when we did hand calculations for hypothesis testing. At the very bottom of the two-tailed, .05 alpha t-test with a large degrees of freedom (z) is 1.96. In other words, 1.96 is the critical value of z when alpha is .05 and we have a two-tailed test. Shapiro-Wilk test In jamovi, we can go to the Explorations option and choose Descriptives. Under statistics, choose Shapiro-Wilk. It will provide you the Shapiro-Wilk W test statistic and its respective p-value. In our case, Shapiro-Wilk’s for height is 68.03, p = .070. If the Shapiro-Wilk’s test is not statistically significant then it is normally distributed. However, if the Shapiro-Wilk’s test is statistically significant then it is not normally distributed. In this case, our Shapiro-Wilk’s test is not statistically significant so we meet the assumption of normal distribution as evidenced by the Shapiro-Wilk’s test. You can learn more about the Shapiro-Wilk test if you are interested. What’s weird about the z-score and Shapiro-Wilk’s test is that we actually want non-statistically significant results here! That’s because the null in these cases is that the data is normally distributed, and we want our data to be normally distributed. So if our p &gt; .05, we fail to reject the null hypothesis, and that suggests the data is normally distributed. Q-Q plot Last, we can visualize the Q-Q plot. In jamovi, we can go to the Explorations option and choose Descriptives. Under plots, choose Q-Q plot. We don’t need to go into details of what is being visualized, but what we are looking for is that the data points fall along the diagonal line. On the figure on the left, we can see that the data is pretty well falling on the diagonal line (with small deviations at the tails) so we can say it looks normally distributed. However, on the figure on the right, the data points deviate from the diagonal line pretty significantly and so we can say it does not look normally distributed. What is enough deviation of the dots from the line? That’s up to you, but typically I focus on what the middle looks like and that the ends don’t look too far off. Here’s a video by Alexander Swan on interpreting a Q-Q plot in jamovi: Homogeneity of variance Our third assumption is that the variance in the DV needs to be the same at each level of the IV. If we fail to meet the assumption, we say we have heterogeneity. It might help you to remember that the prefix homo means same and hetero means different. Whereas the normality assumption is only for the DV, the homogeneity of variance assumption is looking at the DV split by the IV. We can test this assumption in three ways: Visualize the distribution of the DV across the groups of the IV Examine the variance values Levene’s test We are going to learn about three different methods of checking homogeneity of variances. For your homework assignments, you should always use all three methods to check homogeneity of variances. Note that sometimes the three tests do not always agree with one another. For example, everything might look good but the visual distribution does not look like they have equal variances or the Levene’s test might be statistically significant. In those cases, it is up to you as the researcher to make a judgment. Reasonable people do different things, but as long as you can back up your judgment then you’ll be fine. Visualize the distribution of the DV across groups of the IV First, we can look at the data points across groups. This can be done by choosing a plot in the Descriptives analysis and adding your IV to the “Split By” box. Then select Box Plot, Violin, Data (Jittered) under Plots. For example, here’s an example of data that violates the assumption of homogeneity of variance (gender by mile time) because the variance in scores for females (coded as 1) is a lot wider than the variance in scores for males (0). I am looking at the data points and violin plot to see the spread; the 1 looks wider where as the 0 looks skinnier. Examine the variances across groups Similarly, in the Exploration –&gt; Descriptives under Statistics you can ask for Variance after splitting the DV by the IV. The variance for Gender == 0 (male) is 6796.20 whereas the variance for Gender == 1 (female) is 15401.55. Females have 2.26 times greater variance compared to males. Clearly, there is much greater variability for females than males for time it takes to run the mile. Levene’s test When we perform inferential statistics that have the assumption of homogeneity of variance, in jamovi there will be a check box to check the assumption. It will perform Levene’s test. However, you may be testing the assumption prior to running your analysis (or learning about inferential statistics just yet). In that case, to perform Levene’s test, go to Analyses then ANOVA then One-way ANOVA. Move your continuous variable to the Dependent Variables box and move your nominal variable to the Grouping Variable box. Check the box “homogeneity test.” Only report the results of Levene’s test; ignore the one-way ANOVA results! Here’s the result of Levene’s test for the effect of gender on mile duration: Levene’s F df1 df2 p MileMinDur 41.33 1 381 &lt;.001 Like the other tests above, a non-significant Levene’s test means we meet the assumption of homogeneity of variance. However, if Levene’s test is statistically significant, then we fail to meet the assumption of homogeneity of variance and have heterogeneity of variance. In this case, our test is statistically significant so, in combination with our plot above, we say we violated this assumption. Just like the z-score and Shapiro-Wilk’s test, for Levene’s test we actually want non-statistically significant results here! That’s because the null is that there is homogeneity of variance, and we want our data to be homogeneous. So if our p &gt; .05, we fail to reject the null hypothesis, and that suggests the data is homogeneous. Recapping parametric assumptions Let’s summarize the four assumptions and generally how to test for them: Interval or ratio (i.e., continuous) dependent variable Know your levels of measurement and make sure the dependent variable is continuous. Independent scores on the dependent variable Know your research design and how the data was collected. Make sure that one participant’s data isn’t thought to affect another participant’s data. Normal distribution of the dependent variable Visualize the distribution with a histogram and/or density plot of the dependent variable Test the skew and kurtosis by calculating the z-scores Test using the Shapiro-Wilk test Examine the Q-Q plot for deviations from the diagonal line Homogeneity of variances Visualize the distribution by a boxplot, violin plot, and/or data (jittered recommended) with the dependent variable split by the independent variable Examine the variances of the dependent variable split by the independent variable Test using Levene’s test "],["violated-assumptions.html", "9.3 Violated assumptions", " 9.3 Violated assumptions What do you do if you have violated assumptions? Let’s first talk about the assumptions we don’t test (interval/ratio data and independent scores) before we turn to the other two assumptions (normality and homogeneity of variance). Interval/ratio data If you are trying to perform a statistical test with a categorical DV, the answer is simple: perform the test that requires a categorical DV and do not try to treat it as continuous. For example, if you have an ordinal DV and a categorical IV, you can perform a chi-square. If you have an ordinal DV and a continuous IV, you can perform a logistic regression (we won’t be covering that in this class). Go back to the section on choosing the correct statistical test and you’ll see four options of statistical tests that can be performed with a categorical DV. Independent data We won’t be covering it in this class, but if you violate this assumption then you need to use a statistical test that accounts for nested data or can correlate the errors among dependent data. For example, multilevel modeling (aka hierarchical modeling) is one approach. Normality or homogeneity of variance If you violate either normality or homogeneity of variance, there are a few options you can choose. Something I commonly hear from students when they read this section is whether this isn’t messing with our data and making it invalid. However, outliers and non-normal data do not play nicely with our inferential statistics, so what we are doing is making our data more appropriate for our tests. Whatever changes we make should be transparently reported so that our readers and reviewers can critically examine our methods. Remove outliers First, double check that you do not have any outliers. How do you know if data is an outlier? To look for outliers in single variables (aka univariate outliers), you can just look at your data. To look for multivariate outliers (outliers across multiple variables), you can look at Mahalanobis distance or Cook’s distance, which you would need to use the Rj editor to perform in jamovi and we won’t cover in this class. What can you do in case of outliers? Ignore them, but this is not a good solution Delete the outlier cases, but this is not recommended either because you lose data and we now know how important it is to have a large sample size Winsorize, trim, or modify your data, especially if there are only a few outliers Transform the variable, especially if there are a lot of outliers Winsorize or trim the data Winsorizing is used when both tails of the distribution have outliers whereas trimming is used when it’s just one or a few outliers on one side of the distribution. In both cases, we replace the extreme valuse with the next-most-extreme values. There’s more to Winsorizing than what I’ve described here, so I encourage you to learn more if you are interested. We can do this through the Transform feature on jamovi. For example, here’s what it looks like to trim the Reading variable to get rid of the few scores on the far left of the histogram. We want to take those values less than 60 and replace their scores with a new score of 60. If we were winsorizing on both ends, we might take values greater than 100 and replace with 100. Transforming data If we violate the assumption of normality or homogeneity of variance (or both!) then we can explore whether transformations of the entire variable can improve the normality of our data. There are a variety of different transformations you can try, and here’s a list of a few: Name Syntax Corrects for positive skew? Corrects for negative skew? Corrects for unequal variances? Log log(X) Yes No Yes Square Root sqrt(X) Yes No Yes Reciprocal 1/X Yes No Yes Reverse Score (1+MAX) - X then do one of the above transformations No Yes No When you perform a transformation, then you need to check whether the transformation actually improved the situation. How do you do that? Check normality and homogeneity of variance again with your newly transformed data! You should check with the 4 methods to test for normality and 2 methods to test for homogeneity of variance. Non-parametric tests If all else fails–meaning there are no outliers or no transformations fix the violated assumption(s)–then you can perform a non-parametric test. These tests have no assumption of normally distributed data or homogeneity of variance! As you saw in the chart about choosing the correct statistical test, many of our parametric tests have non-parametric equivalents. You might wonder why we don’t just use non-parametric tests and just get rid of all these parametric tests. However, if we meet our assumptions then the parametric test will be more capable of detecting effects if they exist compared to non-parametric tests. When we don’t meet assumptions, then the non-parametric tests will be more capable of detecting effects. When we cover each individual statistical test (e.g., independent t-test) we will also cover its non-parametric equivalent (e.g., Mann-Whitney test). So stay tuned and just remember you have this option if you violate assumptions! "],["writing-up-results-in-apa-style.html", "10. Writing up results in APA style", " 10. Writing up results in APA style This chapter goes over how to write up statistical results. Refer back to this chapter often! The 4 components of writing statistical results When writing up the results of a statistical test, we should always include the following information: Description of your research question and/or hypotheses. Description of your data. If you fail to meet assumptions, you should specify that and describe what test you chose to perform as a result. The results of the inferential test, including what test was performed, the test value and degrees of freedom, p-value, and effect size. Interpretation of the results or whether the hypothesis was supported or not, including any other information as needed. An example Let’s go through an example. This data comes from the independent t-test chapter. Here’s the four pieces of information we need to know: Research question or hypothesis: Is there a difference in student grades between Anastasia’s and Bernadette’s classes? Descriptive statistics: Anastasia’s students (M = 74.53, SD = 9.00, n = 15); Bernadette’s students (M = 69.06, SD = 5.77, n = 18) Inferential statistics: t (31) = 2.12, p = .043, d = .74 Interpretation of results that answer’s the research question or demonstrates whether the hypothesis was rejected or not: Anastasia’s students had significantly higher grades than Bernadette’s students. Given the results of the t-test, we can write up our results in APA something like this: The research question was whether there was a difference in student grades between Anastasia’s and Bernadette’s classes. Anastasia’s students (M = 74.53, SD = 9.00, n = 15) had significantly higher grades than Bernadette’s students (M = 69.06, SD = 5.77, n = 18), t (31) = 2.12, p = .043, d = .74. Let’s compare that write-up against the 4 things we need to report: #1: The research question was whether there was a difference in student grades between Anastasia’s and Bernadette’s classes. #4 Anastasia’s students #2 (M = 74.53, SD = 9.00, n = 15) #4 cont. had significantly higher grades than Bernadette’s students #2 (M = 69.06, SD = 5.77, n = 18), #3 t (31) = 2.12, p = .043, d = .74. Sometimes, people like to put the statistics inside a parentheses. In that case, you need to change the parentheses around the degrees of freedom as brackets. Here’s another example write-up of the results in APA style: #1 I tested the difference in grades between Anastasia’s students #2 (M = 74.53, SD = 9.00, n = 15) and Bernadette’s students (M = 69.06, SD = 5.77, n = 18). #3 An independent samples t-test showed that the 5.48 mean difference between the tutor’s student was statistically significant (t [31] = 2.12, p = .043, d = .74). #4 Therefore, we reject the null hypothesis that there is no difference in grades between the two classes. Note that these are not the only way we can write up the results in APA format. The key is that we include all four pieces of information as specified above, that the information is correct, and that the information is presented in a logical order based on how we test hypotheses. One tip I strongly recommend when writing up your results is to read out your results minus everything in parentheses to check for correct grammar. 7 Common Errors The following video details 7 common errors that folks make when reporting statistical results: Italics: italicize all the statistical letters (e.g., M, SD, p, t, r, etc.) but not Greek letters Decimal places: round everything to 2 decimals except p-values which are rounded to 3 decimals Leading zero: no leading zeros for p-values, correlation, eta-squared, and betas, but leading zeros for everything else p-values: report exact p-values, and if values are less than .001 we report it as p &lt; .001 (never as p &lt;= .001!) Parentheses: we cannot put parentheses inside parentheses; in those cases, put a bracket inside a parenthesis (e.g., here’s a parentheses with [something] in brackets) N (populations or the full study sample) or n (samples, or a sub-sample within a study) Blanks and F: to aid readability, there is no space between the statistcal test and the parentheses for the degrees of freedom, but for the F-statistic there is a space between the two degrees of freedom, such as F(2, 126) "],["chi-square-overview.html", "11. Chi-square overview", " 11. Chi-square overview The chi-square (pronounced “kai”) is a categorical data analysis such that you are only analyzing categorical data. There are two basic types of chi-square tests we’ll be covering: \\(\\chi^2\\) goodness-of-fit: used with one variable to find if the observed frequencies match the expected frequencies \\(\\chi^2\\) test of independence (or association): used with two variables to find if the observed frequencies match the expected frequencies. In other words, are the two nominal variables independent or associated with one another? Fisher’s exact test: This is an alternative to the \\(\\chi^2\\) test of independence that we use when our frequencies are small. McNemar’s test: This is an alternative to the \\(\\chi^2\\) test of independence that we have a 2x2 repeated-measures design. For example, perhaps we examine pass/fail rates before and after a training. Because these tests are all with categorical data, there are no assumptions about the distribution of the data. For that reason, these are all non-parametric statistics. Entering data in jamovi One thing to note that is unique about the chi-square is that you can either setup your data in the raw format or you can use the frequency tables as your data. If you use the frequency table, then you can move the counts/frequency variable into the Counts box in either the goodness-of-fit or test of independence analyses. "],["chi-square-goodness-of-fit.html", "12. Chi-square goodness-of-fit", " 12. Chi-square goodness-of-fit The \\(\\chi^2\\) (chi-square) goodness-of-fit tests whether an observed frequency distribution of a nominal variable matches an expected frequency distribution. Our general hypotheses for the chi-square goodness-of-fit test is as follows: \\(H_0\\): The observed frequencies match the expected frequencies. In other words, the frequencies of the categories of the independent variable are what we would expect. \\(H_1\\): At least one observed frequency doesn’t match the expected frequency. In other words, the frequencies of at least one category of the independent variable are not what we would expect. Note that these are not how you should report your hypotheses! You should specify your hypotheses in relation to the nature of your data. For example, if we have a deck of cards and want to see if people don’t choose cards randomly, the null hypothesis would be that there is a 25% probability of getting each hearts, clubs, spades, and diamonds. There is no directionality for a chi-square test, so there is no such thing as a one-tailed or two-tailed hypothesis. "],["step-1-look-at-the-data.html", "Step 1: Look at the data", " Step 1: Look at the data Let’s run an example with data from lsj-data. Open data from your Data Library in “lsj-data”. Select and open “randomness”. This dataset has participants pull two cards from a deck. For now, we’re just going to work with choice_1. We’re interested in finding out if participants pull cards randomly from the deck. Here’s a video walking through the chi-square goodness of fit test example for this chapter. Data set-up Our data set-up for a chi-square goodness-of-fit test is pretty simple, We just need a single column with the nominal categories of the independent variable that each participant is in. Describe your data Once we confirm our data is setup correctly in jamovi, we should look at our data using descriptive statistics and graphs. First, our descriptive statistics are shown below. With nominal variables like choice_1, we should request Frequency tables, not descriptive statistics like the mean and median. The mean for choice_1 would be, quite frankly, meaningless. What’s the average card type? It can’t exist. So we do frequencies instead. Notice how jamovi is pretty smart here and knows not to give us the mean, median, minimum, and maximum. Check the box for Frequency tables to receive those. From our data, we see that most participants pulled a hearts card first (n = 64, 32%) followed by diamonds (n = 51, 26%), spades (n = 50, 25%), and finally clubs (n = 35, 18%). A bar plot can visualize these descriptive statistics nicely. Specify the hypotheses We’re interested in finding out if participants pull cards randomly from a deck of cards. A typical deck of cards has 52 cards, 13 for each of the four suits (clubs, spades, hearts, diamonds). Because there are 4 suits, then 1/4 is 25% which is our expected frequency of pulling cards randomly from the deck. \\(H_0\\): Participants pull cards randomly from the deck. In other words, there is a 25% probability of pulling each hearts, clubs, spades, and diamonds. \\(H_1\\): Participants pull cards not at random from the deck. In other words, participants have a different probability than 25% of pulling at least one of the types of cards. "],["step-2-check-assumptions.html", "Step 2: Check assumptions", " Step 2: Check assumptions The chi-square goodness-of-fit test has just one assumption: Expected frequencies are sufficiently large, which is usually greater than 5. You test for this assumption by checking the “Expected counts” box. You will then see rows of expected counts in your contingency table. Look at the “expected” numbers and check that they are all 5 or greater. "],["step-3-perform-the-test.html", "Step 3: Perform the test", " Step 3: Perform the test To perform the chi-square goodness of fit test, do the following steps: Go to the Analyses tab, click the Frequencies button, and choose “One sample proportion tests - N outcomes”. Move your variable into the Variable box. In this case, move choice_1 into the Variable box. Select Expected counts so you can check for your assumption of expected frequencies. When you are done, your setup should look like this: Expected proportions: Different Expected Frequencies As you will see in the output, jamovi automatically assumed equal proportions of frequencies (in this case 1/4 or 25% chance of pulling each card). However, there might be times when you want to test a different null hypothesis. Maybe we’re testing the whether our sample frequencies match the population frequencies and those are uneven (e.g., whether our 40/60 gender split matches the 36/64 gender split in the population). We can use the Expected Proportions in the setup to specify different expected frequencies. Note that this is not us checking the assumption of expected frequencies! For example, in our current scenario maybe we think our deck is a little stacked in favor of red cards–or we think our participants are more likely to choose red cards than black cards. We can specify our expected proportions and then interpret the results, such as the example shown in the figure below. In this case, assuming a null distribution of a stacked deck for red cards, participants do not seem more likely to choose red cards based on the expected frequencies we provided. "],["step-4-interpret-results.html", "Step 4: Interpret results", " Step 4: Interpret results The first table shows us our observed frequencies (our data) and expected frequencies (N/k = 200/4 = 50 which is 25% for each one, like we previously calculated). The second table gives us our results. Our p-value is less than our alpha of .05 so we can reject the null hypothesis that the observed frequencies match our expected frequencies. The degrees of freedom (df) is the number of categories minus one (in this case, 4-1 = 3). Note that the chi-square goodness of fit test does not provide us with any measures of effect size. Write up the results in APA style We can write up our results in APA something like this: Of the 200 participants in the experiment, 64 selected hearts for their first choice, 51 selected diamonds, 50 selected spades, and 35 selected clubs. A chi-square goodness-of-fit test was conducted to test whether the choice probabilities were identical for all four suits. The results were statistically significant (\\(\\chi^2\\) [3] = 8.44; p = .038), suggesting that people did not select suits purely at random. Participants chose the hearts (32%) more frequently than expected and the clubs (17%) less frequently than expected. Note that I described the data in the first sentence, but I could have also described it in more detail in the last sentence as part of my interpretation or I could have even written up the results in a table! There are many ways to write up results in APA style, and what’s important is you have the four pieces of information as described in the Writing Results chapter of this textbook. Visualize the results The bar plot we can get using Exploration - Descriptives (as shown above in Describe your data) does a decent job of visualizing the results. You can use the default bar plot in jamovi for your assignments. If you want to take your visualizations up a notch, we can also do this fairly simply in Excel. Instead of counts, perhaps we care more about percentages, and adding a line for the expected frequency (25%). Here’s an example, using instructions from this tutorial: "],["additional-practice.html", "Additional practice", " Additional practice One additional practice you can conduct is performing the same analysis as above but with choice_2! However, if you want to see how choice_1 relates to choice_2 then we need to perform the chi-square test of independence, which is in the next chapter. Another additional practice is in our Sample Dataset. Open the Sample_Dataset_2014.xlsx file that we will be using for all Your Turn exercises. You can find the dataset here: Sample_Dataset_2014.xlsx Download To get the most out of these exercises, try to first find out the answer on your own and then go to the appendix to check your answer. Are there equal numbers of athletes and non-athletes? (Athlete variable) Do you meet the assumptions? Are the observed frequencies similar to the expected frequencies? What is your chi-square value, rounded to two decimal places? I happen to know the school this data comes from has 40% athletes and 60% non-athletes. Does our data match the school population? Change your Expected Proportions ratio to .6 for non-athletes and .4 for athletes. Are the observed frequencies similar to the expected frequencies? What is your chi-square value, rounded to two decimal places? Are there equal numbers of freshmen, sophomores, juniors, and seniors? (Rank variable) Do you meet the assumptions? Are the observed frequencies similar to the expected frequencies? What is your chi-square value, rounded to two decimal places? "],["chi-square-test-of-independence.html", "13. Chi-square test of independence", " 13. Chi-square test of independence The \\(\\chi^2\\) (chi-square) test of independence (or association) tests whether an observed frequency distribution of a nominal variable matches an expected frequency distribution, but unlike the goodness-of-fit test we are looking at the relationship, independence, or association between two variables. Our basic hypotheses for the chi-square test of indepdendence is as follows: \\(H_0\\): There are no differences in frequencies of how the categories in one variable relate to the levels in another variable. \\(H_1\\): At least one category in one variable has significantly different frequencies in another variable than we would expect. Note that these are not how you should report your hypotheses! You should specify your hypotheses in relation to the nature of your two variables. It should be clear what your two variables are. There is no directionality for a chi-square test, so there is no such thing as a one-tailed or two-tailed hypothesis. We often communicate what kind of chi-square test we are performing by using language like “We performed a 2x3 chi-square test of independence.” This means that there are two variables, the first of which has 2 categories to the categorical variable and the second of which has 3 categories to the categorical variable. We might describe it in even more detail, such as “We performed a 2 (condition: experimental vs control) x 3 (mood: happy, sad, or neutral) chi-square test of independence.” "],["step-1-look-at-the-data-1.html", "Step 1: Look at the data", " Step 1: Look at the data Let’s run an example with data from lsj-data. Open data from your Data Library in “lsj-data”. Select and open “chapek9”. This dataset indicates the ID number of the participant, the species (robot or human), and their preference of the three things (puppy, flower, or data). For this example, imagine we are watching a show about the planet Chapek 9. On this planet, for someone to gain access to their capital city they must prove they’re a robot, not a human. In order to determine whether or not a visitor is human, the natives ask whether the visitor prefers puppies, flowers, or large, properly formatted data files. This is what we would call a 2x3 chi-square test because the first variable species has two categories (robot or human) and the second variable choice has three categories (puppies, flowers, data). Here’s a video walking through the chi-square test of independence example in this chapter. Data set-up Our data set-up for a chi-square test of independence is pretty simple. We just need two columns of nominal data, with one row per participant. Here’s our data for our example we’ll be working with, which you can find in the lsj-data called chapek9: ID species choice 1 robot flower 2 human data 3 human data 4 human data 5 robot data 6 human flower 7 human data 8 robot data 9 human puppy 10 robot flower Describe the data Once we confirm our data is setup correctly in jamovi, we should look at our data using descriptive statistics and graphs. First, our descriptive statistics are shown below. Remember that for nominal variables we should report frequency statistics, not means and medians and such. Bar plots continue to be a good way of visualizing the data. Specify the hypotheses The question here is whether humans and robots differ in preferring puppies, flowers, or data so we can determine who is a robot so only robots are let into the city. Therefore, our hypotheses might be something like this: \\(H_0\\): Humans and robots have similar preferences. \\(H_1\\): Humans and robots have different preferences. "],["step-2-check-assumptions-1.html", "Step 2: Check assumptions", " Step 2: Check assumptions The chi-square test of independence has the following assumptions: Expected frequencies are sufficiently large, which is usually greater than 5. If you violate this assumption, you can use Fisher’s exact test. You test for this assumption by selecting “Expected counts” in the Cells tab for the test of independence. You will then see rows of expected counts in your contingency table. Look at the numbers and check that they are all 5 or greater. Data are independent of one another, meaning each case contributes to only one cell of the table. If you violate this assumption, you may be able to use the McNemar test (next chapter). This requires knowing how your data was collected. If it’s a within-subjects design (i.e., all participants are in all conditions of one variable), then most likely you want to use McNemar’s test. If it’s a between-subjects design (i.e., some participants are in one condition of a variable), then you most likely meet this assumption and can perform the chi-square test of independence. "],["step-3-perform-the-test-1.html", "Step 3: Perform the test", " Step 3: Perform the test Go to the Analyses tab, click the Frequencies button, and choose “Independent Samples - \\(\\chi^2\\) test of association”. Move your two variables into the rows and columns boxes. In this case, move choice into rows and species into columns. Note that the placement in rows or columns doesn’t really matter, but because we typically work with portrait pages I tend to prefer putting in rows whatever variable has more categories. In this case, choice has 3 categories and species only 2 so I like to put choice in rows and species in columns. Under the Statistics tab, select \\(\\chi^2\\) under Tests and Phi and Cramer's V under Nominal to get your effect size. Select Expected counts under Cells to test your assumption of expected frequencies. Optionally, you can request the row, column, and total percentages. I often find these easier to report and interpret. Select Bar Plot under plots. You may want to tinker with the settings here of determining whether you should use a side by side or stacked bar type, counts or percentages, and rows or columns. When you are done, part of your setup should look like this (note: not all the setup is shown here!): Ordinal variable(s) If either of your variables are ordinal, instead of selecting Phi and Cramer's V you should select Gamma or Kendall's tau-b. Which do you choose? Kendall's tau-b should only be chosen if you have a square table (e.g., 3x3, 4x4, 5x5) whereas Gamma can be done with any size table. Kendall's tau-b will be a slightly more conservative estimate compared to Gamma. "],["step-4-interpret-results-1.html", "Step 4: Interpret results", " Step 4: Interpret results The first table shows us our observed and expected frequencies. We use the expected frequencies to test our assumption that expected frequencies are greater than 5. Our smallest expected frequncy is 13.53 so we meet this assumption. The second table gives us our results. Our p-value (p = .005) is less than .05 so we can reject the null hypothesis that the observed frequencies match our expected frequencies. The degrees of freedom (df) is the number of categories of one variable minus one times the number of categories in the second variable minus one which is (r - 1)(c - 1) or in this case (2-1)(3-1) = 2. jamovi also gives us our Cramer’s V value. Note that it does not provide Phi because we don’t have a perfect square table (e.g., 2x2 or 3x3). These are measures of effect size for the chi-square. Cramer’s V can be interpreted similar to a correlation (ranges from 0 to 1, with higher scores meaning stronger relationships between the variables). Currently, jamovi cannot tell us where the differences are if the chi-square is statistically significant. That means that the expected frequencies in one cell differ significantly from the observed frequencies, but which cells? For now, just eyeball the comparisons if the chi-square is statistically significant. Someday I’m sure jamovi will include standardized residuals for us to test this more quantitatively. Write up the results in APA style We can write up our results in APA something like this: Pearson’s \\(\\chi^2\\) test of independence showed a significant association between species and choice, \\(\\chi^2\\) (2) = 10.72, p = .005, Cramer’s V = .24. Robots were more likely to say they prefer flowers (70%) compared to humans (30%) and humans were more likely to say they prefer data (60%) compared to robots (40%). Robots (46%) and humans (54%) were equally likely to prefer puppies. We can also use the observed frequencies to either visualize (see the next table) or write-up in a table. Here’s an example write-up with a table. Pearson’s \\(\\chi^2\\) test of independence showed a significant association between species and choice, \\(\\chi^2\\) (2) = 10.72, p = .005, Cramer’s V = .24. Robots were more likely to say they prefer flowers whereas humans were more likely to say they prefer data (see table below). Robots Humans puppy 13 (46%) 15 (54%) flower 30 (70%) 13 (30%) data 44 (40%) 65 (60%) Visualize the results jamovi has some decent plots with some of the latest updates. Here’s one I created after tinkering with the settings under Plots on the chi-square setup. These are sufficient for your homework assignments in this class, but note that sometimes when there are many bars or long labels for the categories of your variable it will scrunch up. Hopefully jamovi will fix it in the future. However, you can also create these pretty easily in Excel. There are two that I think work well for this dataset and our research questions. The first is a clustered column chart: The second is a stacked bar chart with connected lines: Which do you choose? Whichever you think better communicates the message of the results of the analysis! This takes skill and practice, and probably an entirely separate course on effective data visualization. "],["fishers-exact-test.html", "Fisher’s exact test", " Fisher’s exact test If you violate the assumption that your expected frequencies are sufficiently large and you have a 2x2 table, you can still perform the \\(\\chi^2\\) test of independence but instead of selecting \\(\\chi^2\\)you’ll select Fisher's exact test. You’ll interpret your results exactly the same but specify you used Fisher’s exact test. Here’s an example write-up from the results above: Fisher’s exact test of independence showed a significant association between species and choice, \\(\\chi^2\\) (2) = 10.72, p = .004, Cramer’s V = .24. Robots were more likely to say they prefer flowers (70%) compared to humans (30%) and humans were more likely to say they prefer data (60%) compared to robots (40%). Robots (46%) and humans (54%) were equally likely to prefer puppies. Note that what changes is only the p-value from the chi-square test and Fisher’s exact test. "],["additional-practice-1.html", "Additional practice", " Additional practice Open the Sample_Dataset_2014.xlsx file that we will be using for all Your Turn exercises. You can find the dataset here: Sample_Dataset_2014.xlsx Download To get the most out of these exercises, try to first find out the answer on your own and then go to the appendix to check your answer. Is Athlete related to Gender? Do you meet the assumptions? Which test should you perform? Are the observed frequencies similar to the expected frequencies? What is your chi-square value, rounded to two decimal places? Is Gender related to Rank? Do you meet the assumptions? Which test should you perform? Are the observed frequencies similar to the expected frequencies? What is your chi-square value, rounded to two decimal places? "],["mcnemars-test.html", "14. McNemar’s test", " 14. McNemar’s test McNemar’s test is based on the \\(\\chi^2\\) (chi-square) test of independence (or association), but is used in a repeated measures or within-subjects design. We’ll go over a brief example so you’re familiar with this test, but in practice I don’t see this statistic often so we won’t go over it any further than this. "],["step-1-look-at-the-data-2.html", "Step 1: Look at the data", " Step 1: Look at the data For example, suppose we’re working with the Australian Generic Political Party (AGPP) and your job is to find out how effective AGPP political advertisements are. You gather 100 people and ask them to watch the AGPP ads. You ask participants before and after viewing ads whether they intend to vote for the AGPP. This data comes from lsj-data. Open data from your Data Library in “lsj-data”. Select and open “agpp”. This dataset indicates the ID number of the participant and whether they would vote for AGPP before and after viewing the ads. Data set-up Our data set-up for McNemar’s test is pretty simple. We just need two columns of nominal data, with one row per participant and each column being the same variable at two different time points. Here’s our data for our example we’ll be working with, which you can find in the lsj-data called agpp: ID response_before response_after subj.1 no yes subj.2 yes no subj.3 yes no subj.4 yes no subj.5 no no subj.6 no no subj.7 no no subj.8 no yes subj.9 no no subj.10 no no Specify the hypotheses Given you want to see if the AGPP political advertisements are, you want to see if participants are more likely to vote for AGPP after viewing the advertisement. Therefore, the alternative hypothesis is that intentions to vote for the AGPP don’t match the expected frequency of intentions to vote. "],["step-2-check-assumptions-2.html", "Step 2: Check assumptions", " Step 2: Check assumptions You came here because you violated the assumptions of the test of independence’s assumption of independence. You should have a within-subjects design to perform this test. We meet this assumption so we can move on! "],["step-3-perform-the-test-2.html", "Step 3: Perform the test", " Step 3: Perform the test Go to the Analyses tab, click the Frequencies button, and choose “Paired Samples - McNemar test”. Move response_before into rows and response_after into columns. Note that the placement in rows or columns doesn’t really matter. Under the Statistics tab, select \\(\\chi^2\\) under Tests. Optionally, you can request under to show the row and column percentages. When you are done, your setup should look like this: "],["step-4-interpret-results-2.html", "Step 4: Interpret results", " Step 4: Interpret results The first table shows us our observed frequencies. The second table gives us our results. Our p-value is less than .05 so we can reject the null hypothesis that the observed frequencies match our expected frequencies. Unfortunately, looking at our table it also shows that the ads had a negative effect: people were less likely to vote AGPP after seeing the ads. Write up the results in APA style We can write up our results in APA something like this: McNemar’s test indicated that support for AGPP changed from before to after reviewing the AGPP advertisement, \\(\\chi^2\\) (1) = 13.33, p &lt; .001. Most participants continued to not vote for AGPP after the ad (n = 65) and a few continued to vote for AGPP after the ad (n = 5). However, many participants who originally stated they would vote for AGPP changed to no longer voting for AGPP after the ad (n = 25); only five people who originally would not vote for AGPP changed to vote for AGPP after the ad. "],["reliability.html", "25. Reliability", " 25. Reliability This chapter will eventually discuss reliability testing. Stay tuned! "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
